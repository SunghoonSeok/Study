{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "외부 데이터: 로컬 파일, 드라이브, 스프레드시트, Cloud Storage의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunghoonSeok/Study/blob/master/%EC%99%B8%EB%B6%80_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%A1%9C%EC%BB%AC_%ED%8C%8C%EC%9D%BC%2C_%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C%2C_%EC%8A%A4%ED%94%84%EB%A0%88%EB%93%9C%EC%8B%9C%ED%8A%B8%2C_Cloud_Storage%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z2jcRKwUHqV"
      },
      "source": [
        "이 노트북에서는 외부 소스의 데이터를 로드 및 저장하는 레시피를 이용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "dHBHIPe5Qh0P",
        "outputId": "62dcb197-bf25-4a04-9ea7-b38952340da4"
      },
      "source": [
        "from google.colab import files\r\n",
        "myfile = files.upload()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9df3d278-b055-4690-a0ed-1966a1bf76c7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9df3d278-b055-4690-a0ed-1966a1bf76c7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving KODEX 코스닥150 선물인버스.csv to KODEX 코스닥150 선물인버스.csv\n",
            "Saving 삼성전자.csv to 삼성전자.csv\n",
            "Saving 삼성전자0115.csv to 삼성전자0115.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ_f1CUnVulZ",
        "outputId": "78dd8eaf-cb74-4772-bb26-042facfecc22"
      },
      "source": [
        "import os\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0d1zfCoQ5Lw",
        "outputId": "76c4240b-207b-4f1a-9040-684c2d246ae3"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import io\r\n",
        "# from pandas import read_csv\r\n",
        "# df = read_csv('c:/data/test/삼성전자.csv', index_col=0, header=0)\r\n",
        "df = pd.read_csv(io.BytesIO(myfile['삼성전자.csv']), index_col=0, header=0, encoding='cp949')\r\n",
        "\r\n",
        "\r\n",
        "print(df.shape) # (2400, 14)\r\n",
        "#Index(['시가', '고가', '저가', '종가', '등락률', '거래량', '금액(백만)', '신용비', '개인', '기관',\r\n",
        "#       '외인(수량)', '외국계', '프로그램', '외인비']\r\n",
        "\r\n",
        "# str -> int, 불필요한 행 제거\r\n",
        "df = df.drop(['2018-05-03','2018-05-02','2018-04-30'])\r\n",
        "df['시가'] =df['시가'].str.replace(',','').astype('int64')\r\n",
        "df['고가'] =df['고가'].str.replace(',','').astype('int64')\r\n",
        "df['저가'] =df['저가'].str.replace(',','').astype('int64')\r\n",
        "df['종가'] =df['종가'].str.replace(',','').astype('int64')\r\n",
        "df['거래량'] =df['거래량'].str.replace(',','').astype('int64')\r\n",
        "df['금액(백만)'] =df['금액(백만)'].str.replace(',','').astype('int64')\r\n",
        "df['개인'] =df['개인'].str.replace(',','').astype('int64')\r\n",
        "df['기관'] =df['기관'].str.replace(',','').astype('int64')\r\n",
        "df['외인(수량)'] =df['외인(수량)'].str.replace(',','').astype('int64')\r\n",
        "df['외국계'] =df['외국계'].str.replace(',','').astype('int64')\r\n",
        "df['프로그램'] =df['프로그램'].str.replace(',','').astype('int64')\r\n",
        "print(df.shape)\r\n",
        "\r\n",
        "# 데이터 추가, str -> float\r\n",
        "# df2 = read_csv('c:/data/test/삼성전자0115.csv', encoding='cp949', index_col=0, header=0, thousands=',')\r\n",
        "df2 = pd.read_csv(io.BytesIO(myfile['삼성전자0115.csv']), encoding='cp949', index_col=0, header=0, thousands=',')\r\n",
        "df2 = df2.dropna()\r\n",
        "df2 = df2.drop(['전일비','Unnamed: 6'], axis=1)\r\n",
        "\r\n",
        "# 중복 데이터 제거\r\n",
        "print(df.shape)\r\n",
        "df = df.drop(['2021-01-13'])\r\n",
        "df2 = df2.drop(df2.index[3:])\r\n",
        "print(df2.shape)\r\n",
        "\r\n",
        "\r\n",
        "# 액면분할 이전 데이터 주가변환\r\n",
        "df = df[::-1]\r\n",
        "df2 = df2[::-1]\r\n",
        "df.loc[:'2018-05-04','시가':'종가'] = (df.loc[:'2018-05-04','시가':'종가'])/50.\r\n",
        "df.loc[:'2018-05-04','거래량'] = (df.loc[:'2018-05-04','거래량'])*50.\r\n",
        "df.loc[:'2018-05-04','개인':'프로그램'] = (df.loc[:'2018-05-04','개인':'프로그램'])*50.\r\n",
        "\r\n",
        "# 데이터 병합\r\n",
        "df = pd.concat([df, df2], axis=0)\r\n",
        "print(df.shape)\r\n",
        "print(df.tail())\r\n",
        "\r\n",
        "# 타겟(y값) 설정\r\n",
        "dataset_y = df.iloc[:,0]\r\n",
        "df['Target'] = dataset_y\r\n",
        "\r\n",
        "# 불필요한 특성 제거\r\n",
        "# df = df.drop(['거래량','신용비','외인비'], axis=1)\r\n",
        "# df = df.drop(['개인','기관','외인(수량)','외국계','프로그램'], axis=1)\r\n",
        "print(df.shape)\r\n",
        "\r\n",
        "# csv -> npy 후 저장\r\n",
        "data = df.values\r\n",
        "print(data)\r\n",
        "# np.save('c:/data/test/samsung_jusik2.npy', arr=data)\r\n",
        "np.save('/content/drive/My Drive/data/test/samsung_jusik.npy', arr=data)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2400, 14)\n",
            "(2397, 14)\n",
            "(2397, 14)\n",
            "(3, 14)\n",
            "(2399, 14)\n",
            "                 시가       고가       저가  ...        외국계       프로그램    외인비\n",
            "일자                                     ...                             \n",
            "2021-01-11  90000.0  96800.0  89500.0  ... -4979740.0 -6795684.0  55.59\n",
            "2021-01-12  90300.0  91400.0  87800.0  ... -2093652.0 -4498684.0  55.56\n",
            "2021/01/13  89800.0  91200.0  89100.0  ... -2774590.0 -2190774.0  55.52\n",
            "2021/01/14  88700.0  90000.0  88700.0  ...  2193784.0 -1091335.0  55.57\n",
            "2021/01/15  89800.0  91800.0  88000.0  ...  -261904.0 -3522801.0  55.57\n",
            "\n",
            "[5 rows x 14 columns]\n",
            "(2399, 15)\n",
            "[[ 1.782000e+04  1.784000e+04  1.732000e+04 ...  3.567500e+05\n",
            "   5.088000e+01  1.782000e+04]\n",
            " [ 1.706000e+04  1.750000e+04  1.706000e+04 ... -7.192500e+05\n",
            "   5.090000e+01  1.706000e+04]\n",
            " [ 1.778000e+04  1.832000e+04  1.772000e+04 ...  3.235600e+06\n",
            "   5.093000e+01  1.778000e+04]\n",
            " ...\n",
            " [ 8.980000e+04  9.120000e+04  8.910000e+04 ... -2.190774e+06\n",
            "   5.552000e+01  8.980000e+04]\n",
            " [ 8.870000e+04  9.000000e+04  8.870000e+04 ... -1.091335e+06\n",
            "   5.557000e+01  8.870000e+04]\n",
            " [ 8.980000e+04  9.180000e+04  8.800000e+04 ... -3.522801e+06\n",
            "   5.557000e+01  8.980000e+04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8pTtjPGXHyr",
        "outputId": "52e28221-29bc-4394-9f9b-e9ba2cb70c7b"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import io\r\n",
        "# from pandas import read_csv\r\n",
        "# df = read_csv('c:/data/test/삼성전자.csv', index_col=0, header=0)\r\n",
        "df = pd.read_csv(io.BytesIO(myfile['KODEX 코스닥150 선물인버스.csv']), index_col=0, header=0, encoding='cp949', thousands=',')\r\n",
        "\r\n",
        "\r\n",
        "print(df.shape) # (2400, 14)\r\n",
        "#Index(['시가', '고가', '저가', '종가', '등락률', '거래량', '금액(백만)', '신용비', '개인', '기관',\r\n",
        "#       '외인(수량)', '외국계', '프로그램', '외인비']\r\n",
        "\r\n",
        "# # str -> int, 불필요한 행 제거\r\n",
        "df = df.drop(['2018/05/03','2018/05/02','2018/04/30'])\r\n",
        "# df['시가'] =df['시가'].str.replace(',','').astype('int64')\r\n",
        "# df['고가'] =df['고가'].str.replace(',','').astype('int64')\r\n",
        "# df['저가'] =df['저가'].str.replace(',','').astype('int64')\r\n",
        "# df['종가'] =df['종가'].str.replace(',','').astype('int64')\r\n",
        "# df['거래량'] =df['거래량'].str.replace(',','').astype('int64')\r\n",
        "# df['금액(백만)'] =df['금액(백만)'].str.replace(',','').astype('int64')\r\n",
        "# df['개인'] =df['개인'].str.replace(',','').astype('int64')\r\n",
        "# df['기관'] =df['기관'].str.replace(',','').astype('int64')\r\n",
        "# df['외인(수량)'] =df['외인(수량)'].str.replace(',','').astype('int64')\r\n",
        "# df['외국계'] =df['외국계'].str.replace(',','').astype('int64')\r\n",
        "# df['프로그램'] =df['프로그램'].str.replace(',','').astype('int64')\r\n",
        "# print(df.shape)\r\n",
        "\r\n",
        "# 데이터 추가, str -> float\r\n",
        "# df2 = read_csv('c:/data/test/삼성전자0115.csv', encoding='cp949', index_col=0, header=0, thousands=',')\r\n",
        "# df2 = pd.read_csv(io.BytesIO(myfile['삼성전자0115.csv']), encoding='cp949', index_col=0, header=0, thousands=',')\r\n",
        "df = df.dropna()\r\n",
        "df = df.drop(['전일비','Unnamed: 6'], axis=1)\r\n",
        "\r\n",
        "# # 중복 데이터 제거\r\n",
        "# print(df.shape)\r\n",
        "# df = df.drop(['2021-01-13'])\r\n",
        "# df2 = df2.drop(df2.index[3:])\r\n",
        "# print(df2.shape)\r\n",
        "df = df[::-1]\r\n",
        "\r\n",
        "# # 액면분할 이전 데이터 주가변환\r\n",
        "# df = df[::-1]\r\n",
        "# df2 = df2[::-1]\r\n",
        "# df.loc[:'2018-05-04','시가':'종가'] = (df.loc[:'2018-05-04','시가':'종가'])/50.\r\n",
        "# df.loc[:'2018-05-04','거래량'] = (df.loc[:'2018-05-04','거래량'])*50.\r\n",
        "# df.loc[:'2018-05-04','개인':'프로그램'] = (df.loc[:'2018-05-04','개인':'프로그램'])*50.\r\n",
        "\r\n",
        "# # 데이터 병합\r\n",
        "# df = pd.concat([df, df2], axis=0)\r\n",
        "# print(df.shape)\r\n",
        "# print(df.tail())\r\n",
        "\r\n",
        "# # 타겟(y값) 설정\r\n",
        "# dataset_y = df.iloc[:,0]\r\n",
        "# df['Target'] = dataset_y\r\n",
        "\r\n",
        "# 불필요한 특성 제거\r\n",
        "# df = df.drop(['거래량','신용비','외인비'], axis=1)\r\n",
        "# df = df.drop(['개인','기관','외인(수량)','외국계','프로그램'], axis=1)\r\n",
        "print(df.shape)\r\n",
        "\r\n",
        "# csv -> npy 후 저장\r\n",
        "data = df.values\r\n",
        "print(data)\r\n",
        "# np.save('c:/data/test/samsung_jusik2.npy', arr=data)\r\n",
        "np.save('/content/drive/My Drive/data/test/kodex_jusik.npy', arr=data)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1088, 16)\n",
            "(1085, 14)\n",
            "[[1.0000e+04 1.0000e+04 9.8650e+03 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [9.8900e+03 1.0025e+04 9.8650e+03 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [1.0020e+04 1.0025e+04 9.9450e+03 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " ...\n",
            " [4.3700e+03 4.4100e+03 4.3350e+03 ... 0.0000e+00 0.0000e+00 4.4000e+00]\n",
            " [4.4000e+03 4.4400e+03 4.3800e+03 ... 0.0000e+00 0.0000e+00 4.5400e+00]\n",
            " [4.4200e+03 4.5450e+03 4.4050e+03 ... 0.0000e+00 0.0000e+00 4.5200e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD9J1-TOYJTn",
        "outputId": "e7b50f06-f4cd-40b2-9b3c-179529194111"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# 1. 데이터\r\n",
        "data1 = np.load('/content/drive/My Drive/data/test/samsung_jusik.npy')\r\n",
        "data2 = np.load('/content/drive/My Drive/data/test/kodex_jusik.npy')\r\n",
        "\r\n",
        "x1 = data1[1314:,:-1]\r\n",
        "x2 = data2\r\n",
        "y = data1[1314:,-1]\r\n",
        "print(x1.shape, x2.shape)\r\n",
        "print(y.shape)\r\n",
        "\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "scaler1 = MinMaxScaler()\r\n",
        "scaler1.fit(x1)\r\n",
        "x1 = scaler1.transform(x1)\r\n",
        "\r\n",
        "scaler2 = MinMaxScaler()\r\n",
        "scaler2.fit(x2)\r\n",
        "x2 = scaler2.transform(x2)\r\n",
        "\r\n",
        "size = 6\r\n",
        "\r\n",
        "def split_x(seq, size):\r\n",
        "    aaa = []\r\n",
        "    for i in range(len(seq)-size+1):\r\n",
        "        subset = seq[i : (i+size)]\r\n",
        "        aaa.append([item for item in subset])\r\n",
        "    print(type(aaa))\r\n",
        "    return np.array(aaa)\r\n",
        "x1_data = split_x(x1, size)\r\n",
        "x2_data = split_x(x2, size)\r\n",
        "\r\n",
        "x1 = x1_data[:-2,:,:]\r\n",
        "x2 = x2_data[:-2,:,:]\r\n",
        "\r\n",
        "\r\n",
        "y = y[7:]\r\n",
        "print(x1.shape, x2.shape)\r\n",
        "print(y.shape)\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "x1_train, x1_test, x2_train, x2_test, y_train, y_test = train_test_split(x1, x2, y, train_size=0.8, shuffle=True, random_state=66)\r\n",
        "\r\n",
        "\r\n",
        "#2. 모델구성\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Dropout, Conv1D, Flatten, MaxPooling1D, GRU, SimpleRNN\r\n",
        "# inputs = Input(shape=(6, x.shape[2]))\r\n",
        "# dense1 = Conv1D(1000, 2, padding='same', activation='relu')(inputs)\r\n",
        "# dense1 = MaxPooling1D(pool_size=2)(dense1)\r\n",
        "# dense1 = Conv1D(500, 2, activation='relu')(dense1)\r\n",
        "# dense1 = Conv1D(400, 2,activation='relu')(dense1)\r\n",
        "# dense1 = Flatten()(dense1)\r\n",
        "\r\n",
        "\r\n",
        "# 모델 1\r\n",
        "input1 = Input(shape=(6,x1.shape[2]))\r\n",
        "dense1 = LSTM(512)(input1)\r\n",
        "dense1 = Dense(256)(dense1)\r\n",
        "dense1 = Dense(128)(dense1)\r\n",
        "dense1 = Dense(64)(dense1)\r\n",
        "dense1 = Dense(32)(dense1)\r\n",
        "dense1 = Dense(16)(dense1)\r\n",
        "dense1 = Dense(8)(dense1)\r\n",
        "dense1 = Dense(4)(dense1)\r\n",
        "dense1 = Dense(2)(dense1)\r\n",
        "\r\n",
        "\r\n",
        "# 모델 2\r\n",
        "input2 = Input(shape=(6,x2.shape[2]))\r\n",
        "dense2 = LSTM(512)(input2)\r\n",
        "dense2 = Dense(256)(dense2)\r\n",
        "dense2 = Dense(128)(dense2)\r\n",
        "dense2 = Dense(64)(dense2)\r\n",
        "dense2 = Dense(32)(dense2)\r\n",
        "dense2 = Dense(16)(dense2)\r\n",
        "dense2 = Dense(8)(dense2)\r\n",
        "dense2 = Dense(4)(dense2)\r\n",
        "dense2 = Dense(2)(dense2)\r\n",
        "\r\n",
        "\r\n",
        "from tensorflow.keras.layers import concatenate, Concatenate\r\n",
        "merge1 = concatenate([dense1, dense2])\r\n",
        "middle1 = Dense(128)(merge1)\r\n",
        "middle1 = Dense(64)(middle1)\r\n",
        "middle1 = Dense(32)(middle1)\r\n",
        "middle1 = Dense(16)(middle1)\r\n",
        "middle1 = Dense(8)(middle1)\r\n",
        "middle1 = Dense(4)(middle1)\r\n",
        "middle1 = Dense(2)(middle1)\r\n",
        "outputs = Dense(1)(middle1)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "model = Model(inputs=[input1,input2], outputs=outputs)\r\n",
        "\r\n",
        "\r\n",
        "#3. 컴파일, 훈련\r\n",
        "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "es = EarlyStopping(monitor='val_loss', patience=100, mode='auto')\r\n",
        "modelpath= '/content/drive/My Drive/data/test/modelcheckpoint/samsung_checkpoint_{val_loss:.4f}.hdf5'\r\n",
        "cp = ModelCheckpoint(modelpath, monitor='val_loss', save_best_only=True, mode='auto')\r\n",
        "model.fit([x1_train,x2_train], y_train, batch_size=64, epochs=1000, validation_split=0.2, callbacks=[cp, es])\r\n",
        "\r\n",
        "model.save('/content/drive/My Drive/data/test/samsung_model2.h5')\r\n",
        "\r\n",
        "#4. 평가, 예측\r\n",
        "loss, mae = model.evaluate([x1_test,x2_test], y_test, batch_size=64)\r\n",
        "y_predict = model.predict([x1_test, x2_test])\r\n",
        "print(\"loss, mae : \", loss, mae)\r\n",
        "\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "def RMSE(y_test, y_predict):\r\n",
        "    return np.sqrt(mean_squared_error(y_test, y_predict))\r\n",
        "print(\"RMSE : \", RMSE(y_test, y_predict))\r\n",
        "\r\n",
        "from sklearn.metrics import r2_score\r\n",
        "r2 = r2_score(y_test, y_predict)\r\n",
        "print(\"R2 : \", r2)\r\n",
        "\r\n",
        "x1_pred = x1_data[-2:,:,:]\r\n",
        "x2_pred = x2_data[-2:,:,:]\r\n",
        "x1_pred = x1_pred.reshape(x1_pred.shape[0],6,x1_pred.shape[-1])\r\n",
        "x2_pred = x2_pred.reshape(x2_pred.shape[0],6,x2_pred.shape[-1])\r\n",
        "y_predict = model.predict([x1_pred,x2_pred])\r\n",
        "\r\n",
        "print(y_predict)\r\n",
        "\r\n",
        "\r\n",
        "print(\"월요일 삼성 시가 : \", int(np.round(y_predict[0])), \"원\")\r\n",
        "print(\"화요일 삼성 시가 : \", int(np.round(y_predict[1])), \"원\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1085, 14) (1085, 14)\n",
            "(1085,)\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "(1078, 6, 14) (1078, 6, 14)\n",
            "(1078,)\n",
            "Epoch 1/1000\n",
            "11/11 [==============================] - 4s 80ms/step - loss: 2328565418.6667 - mae: 47372.1732 - val_loss: 1798982272.0000 - val_mae: 41517.8711\n",
            "Epoch 2/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 1135164842.6667 - mae: 30567.3029 - val_loss: 76268536.0000 - val_mae: 6278.5854\n",
            "Epoch 3/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 185638202.6667 - mae: 10962.1573 - val_loss: 75441528.0000 - val_mae: 6180.7471\n",
            "Epoch 4/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 130507240.6667 - mae: 8719.9219 - val_loss: 127003040.0000 - val_mae: 8813.9824\n",
            "Epoch 5/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 102351230.0000 - mae: 8017.9003 - val_loss: 85704088.0000 - val_mae: 6994.5605\n",
            "Epoch 6/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 78204488.6667 - mae: 6251.8581 - val_loss: 74982488.0000 - val_mae: 6076.2441\n",
            "Epoch 7/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 74126554.3333 - mae: 6247.4404 - val_loss: 75688152.0000 - val_mae: 5973.4341\n",
            "Epoch 8/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 75484253.6667 - mae: 6191.6060 - val_loss: 75063112.0000 - val_mae: 6001.6729\n",
            "Epoch 9/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 72535384.6667 - mae: 6002.2229 - val_loss: 75045424.0000 - val_mae: 6103.0708\n",
            "Epoch 10/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 74736583.0000 - mae: 6153.0079 - val_loss: 75383128.0000 - val_mae: 6171.9487\n",
            "Epoch 11/1000\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 84711869.3333 - mae: 6494.8492 - val_loss: 76979824.0000 - val_mae: 6343.0005\n",
            "Epoch 12/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 71274076.0000 - mae: 6073.1753 - val_loss: 79967968.0000 - val_mae: 6564.9219\n",
            "Epoch 13/1000\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 82514070.6667 - mae: 6774.1687 - val_loss: 76371344.0000 - val_mae: 5989.7993\n",
            "Epoch 14/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 69230240.3333 - mae: 6007.2751 - val_loss: 77258504.0000 - val_mae: 6021.2759\n",
            "Epoch 15/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 80666900.0000 - mae: 6503.8951 - val_loss: 78263696.0000 - val_mae: 6083.2856\n",
            "Epoch 16/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 81141854.0000 - mae: 6557.3148 - val_loss: 76295472.0000 - val_mae: 5987.7104\n",
            "Epoch 17/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 86827816.6667 - mae: 6621.2677 - val_loss: 75413584.0000 - val_mae: 6177.1689\n",
            "Epoch 18/1000\n",
            "11/11 [==============================] - 0s 21ms/step - loss: 72097480.6667 - mae: 6025.0469 - val_loss: 76354632.0000 - val_mae: 6287.1025\n",
            "Epoch 19/1000\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 84634293.3333 - mae: 6665.6904 - val_loss: 75930728.0000 - val_mae: 6243.2471\n",
            "Epoch 20/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 79082544.0000 - mae: 6421.7933 - val_loss: 75397320.0000 - val_mae: 5972.2671\n",
            "Epoch 21/1000\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 80112137.3333 - mae: 6373.0414 - val_loss: 75043480.0000 - val_mae: 6001.6831\n",
            "Epoch 22/1000\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 82078308.3333 - mae: 6418.9704 - val_loss: 75402568.0000 - val_mae: 5971.6714\n",
            "Epoch 23/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 76646356.6667 - mae: 6152.4834 - val_loss: 76301424.0000 - val_mae: 5987.6079\n",
            "Epoch 24/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 80808233.3333 - mae: 6310.3398 - val_loss: 75209416.0000 - val_mae: 5977.8823\n",
            "Epoch 25/1000\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 78598140.3333 - mae: 6275.5803 - val_loss: 74925728.0000 - val_mae: 6060.4604\n",
            "Epoch 26/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 82950662.0000 - mae: 6491.6076 - val_loss: 78863744.0000 - val_mae: 6122.2725\n",
            "Epoch 27/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 74518035.3333 - mae: 6272.5150 - val_loss: 76139832.0000 - val_mae: 5982.6846\n",
            "Epoch 28/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 68707458.6667 - mae: 6024.6500 - val_loss: 75489744.0000 - val_mae: 5968.7080\n",
            "Epoch 29/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 72300658.0000 - mae: 6047.0211 - val_loss: 76197312.0000 - val_mae: 6276.7065\n",
            "Epoch 30/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 80052353.3333 - mae: 6352.5032 - val_loss: 75506176.0000 - val_mae: 5967.8237\n",
            "Epoch 31/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 78939660.6667 - mae: 6391.0494 - val_loss: 75338832.0000 - val_mae: 5966.2798\n",
            "Epoch 32/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 72113208.0000 - mae: 6174.9599 - val_loss: 74885424.0000 - val_mae: 5999.4297\n",
            "Epoch 33/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 87109642.0000 - mae: 6621.1728 - val_loss: 76254864.0000 - val_mae: 5984.1626\n",
            "Epoch 34/1000\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 71429401.3333 - mae: 6318.8723 - val_loss: 76639888.0000 - val_mae: 5997.1211\n",
            "Epoch 35/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 76105146.6667 - mae: 6327.7283 - val_loss: 74761024.0000 - val_mae: 6088.3604\n",
            "Epoch 36/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 74941792.0000 - mae: 6405.2382 - val_loss: 95832704.0000 - val_mae: 7663.3105\n",
            "Epoch 37/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 88512950.0000 - mae: 6788.0602 - val_loss: 74636232.0000 - val_mae: 6010.1157\n",
            "Epoch 38/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 74670749.3333 - mae: 6159.0954 - val_loss: 80419632.0000 - val_mae: 6232.8496\n",
            "Epoch 39/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 79225186.0000 - mae: 6400.3268 - val_loss: 76322096.0000 - val_mae: 5984.0591\n",
            "Epoch 40/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 77543784.0000 - mae: 6312.5179 - val_loss: 74743272.0000 - val_mae: 5953.1064\n",
            "Epoch 41/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 83248714.6667 - mae: 6434.2325 - val_loss: 74799792.0000 - val_mae: 6160.9595\n",
            "Epoch 42/1000\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 74698563.6667 - mae: 6205.8996 - val_loss: 74560544.0000 - val_mae: 6139.8267\n",
            "Epoch 43/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 67171107.3333 - mae: 6001.2523 - val_loss: 73988688.0000 - val_mae: 6029.6904\n",
            "Epoch 44/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 78039598.6667 - mae: 6373.1670 - val_loss: 73894648.0000 - val_mae: 6065.5117\n",
            "Epoch 45/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 85815218.6667 - mae: 6613.3159 - val_loss: 74395704.0000 - val_mae: 5917.7646\n",
            "Epoch 46/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 85499518.6667 - mae: 6608.1473 - val_loss: 73943240.0000 - val_mae: 6137.6567\n",
            "Epoch 47/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 77232459.6667 - mae: 6592.4836 - val_loss: 76618536.0000 - val_mae: 6386.9077\n",
            "Epoch 48/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 75754887.3333 - mae: 6447.1024 - val_loss: 73073176.0000 - val_mae: 6056.8687\n",
            "Epoch 49/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 70713432.0000 - mae: 6240.5345 - val_loss: 76960664.0000 - val_mae: 6053.4712\n",
            "Epoch 50/1000\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 90727130.6667 - mae: 6633.2823 - val_loss: 74126768.0000 - val_mae: 5890.9272\n",
            "Epoch 51/1000\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 67093187.0000 - mae: 5938.9510 - val_loss: 74746824.0000 - val_mae: 6301.2646\n",
            "Epoch 52/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 65886293.3333 - mae: 5856.5677 - val_loss: 72996560.0000 - val_mae: 6186.0591\n",
            "Epoch 53/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 69879937.0000 - mae: 6093.9550 - val_loss: 70597096.0000 - val_mae: 5781.1909\n",
            "Epoch 54/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 65475295.3333 - mae: 6001.6645 - val_loss: 79030160.0000 - val_mae: 6254.4790\n",
            "Epoch 55/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 79000964.6667 - mae: 6402.6382 - val_loss: 67894224.0000 - val_mae: 5773.0825\n",
            "Epoch 56/1000\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 64262467.0000 - mae: 5804.5550 - val_loss: 68060016.0000 - val_mae: 5949.5737\n",
            "Epoch 57/1000\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 78933972.6667 - mae: 6505.6864 - val_loss: 65066480.0000 - val_mae: 5659.6826\n",
            "Epoch 58/1000\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 78157858.0000 - mae: 6367.6527 - val_loss: 63527612.0000 - val_mae: 5518.0757\n",
            "Epoch 59/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 66808323.6667 - mae: 5831.9551 - val_loss: 66613980.0000 - val_mae: 5657.4551\n",
            "Epoch 60/1000\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 65297538.3333 - mae: 6100.4014 - val_loss: 62928120.0000 - val_mae: 5487.3511\n",
            "Epoch 61/1000\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 59197760.3333 - mae: 5690.4344 - val_loss: 61378964.0000 - val_mae: 5838.4312\n",
            "Epoch 62/1000\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 57315023.6667 - mae: 5470.8811 - val_loss: 56370620.0000 - val_mae: 5284.5645\n",
            "Epoch 63/1000\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 55987323.0000 - mae: 5456.8818 - val_loss: 60580384.0000 - val_mae: 6009.2017\n",
            "Epoch 64/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 61462401.3333 - mae: 5566.6671 - val_loss: 55963664.0000 - val_mae: 5318.5640\n",
            "Epoch 65/1000\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 55721340.0000 - mae: 5207.2844 - val_loss: 49728216.0000 - val_mae: 5265.7563\n",
            "Epoch 66/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 42164061.3333 - mae: 4954.7254 - val_loss: 48809900.0000 - val_mae: 5395.8149\n",
            "Epoch 67/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 43162505.3333 - mae: 4962.2262 - val_loss: 62052208.0000 - val_mae: 5960.8037\n",
            "Epoch 68/1000\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 50153240.3333 - mae: 5344.3254 - val_loss: 54819472.0000 - val_mae: 5398.0664\n",
            "Epoch 69/1000\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 43284999.3333 - mae: 4961.0059 - val_loss: 42769180.0000 - val_mae: 4860.6689\n",
            "Epoch 70/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 33686709.6667 - mae: 4428.3338 - val_loss: 44533336.0000 - val_mae: 5087.1860\n",
            "Epoch 71/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 37378627.0000 - mae: 4712.6477 - val_loss: 39672448.0000 - val_mae: 4891.8423\n",
            "Epoch 72/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 35147766.1667 - mae: 4576.1348 - val_loss: 40015508.0000 - val_mae: 4858.7910\n",
            "Epoch 73/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 35753579.8333 - mae: 4542.1071 - val_loss: 39765112.0000 - val_mae: 5052.3569\n",
            "Epoch 74/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 38151499.3333 - mae: 4493.5098 - val_loss: 37642216.0000 - val_mae: 4946.7993\n",
            "Epoch 75/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 33240674.3333 - mae: 4586.0232 - val_loss: 50021756.0000 - val_mae: 5282.4683\n",
            "Epoch 76/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 41277460.3333 - mae: 5044.1965 - val_loss: 52034448.0000 - val_mae: 5934.4746\n",
            "Epoch 77/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 48757403.3333 - mae: 5233.6547 - val_loss: 37325932.0000 - val_mae: 4484.1045\n",
            "Epoch 78/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 32128757.8333 - mae: 4427.2181 - val_loss: 35581672.0000 - val_mae: 4845.7036\n",
            "Epoch 79/1000\n",
            "11/11 [==============================] - 0s 22ms/step - loss: 31309316.1667 - mae: 4280.2984 - val_loss: 38097044.0000 - val_mae: 5001.5854\n",
            "Epoch 80/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 43010917.1667 - mae: 5027.2297 - val_loss: 32612372.0000 - val_mae: 4379.9292\n",
            "Epoch 81/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 32836246.0000 - mae: 4454.9975 - val_loss: 40188308.0000 - val_mae: 5141.3657\n",
            "Epoch 82/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 39677138.3333 - mae: 4899.9423 - val_loss: 47326332.0000 - val_mae: 5670.7476\n",
            "Epoch 83/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 36544432.1667 - mae: 4715.8649 - val_loss: 29549224.0000 - val_mae: 4213.5571\n",
            "Epoch 84/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 30059164.8333 - mae: 4231.3659 - val_loss: 27862190.0000 - val_mae: 4076.2891\n",
            "Epoch 85/1000\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 31060562.8333 - mae: 4057.2009 - val_loss: 30618506.0000 - val_mae: 4441.0566\n",
            "Epoch 86/1000\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 32451889.8333 - mae: 4076.4535 - val_loss: 28925176.0000 - val_mae: 4350.4766\n",
            "Epoch 87/1000\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 34347418.0000 - mae: 4140.7205 - val_loss: 29819140.0000 - val_mae: 4428.7153\n",
            "Epoch 88/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 29387332.0000 - mae: 4242.8474 - val_loss: 53316324.0000 - val_mae: 5939.5532\n",
            "Epoch 89/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 40416619.6667 - mae: 4988.0175 - val_loss: 30210894.0000 - val_mae: 4294.4922\n",
            "Epoch 90/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 34540374.1667 - mae: 4498.0688 - val_loss: 31062238.0000 - val_mae: 4235.6436\n",
            "Epoch 91/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 28509560.5000 - mae: 3955.6439 - val_loss: 27682836.0000 - val_mae: 4212.9028\n",
            "Epoch 92/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 24660192.3333 - mae: 3771.3115 - val_loss: 25926568.0000 - val_mae: 4046.7981\n",
            "Epoch 93/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 24693607.1667 - mae: 3747.6087 - val_loss: 25409282.0000 - val_mae: 4059.8599\n",
            "Epoch 94/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 27480045.0000 - mae: 4091.0514 - val_loss: 24987240.0000 - val_mae: 3952.3872\n",
            "Epoch 95/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 23374683.8333 - mae: 3773.3578 - val_loss: 24975076.0000 - val_mae: 4021.2449\n",
            "Epoch 96/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 26497921.6667 - mae: 3683.7491 - val_loss: 25661452.0000 - val_mae: 4027.2803\n",
            "Epoch 97/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 31394088.6667 - mae: 3879.6357 - val_loss: 26653244.0000 - val_mae: 4127.6787\n",
            "Epoch 98/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 23960240.6667 - mae: 3902.2519 - val_loss: 25642836.0000 - val_mae: 4040.2305\n",
            "Epoch 99/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 24251520.8333 - mae: 3800.6337 - val_loss: 24408616.0000 - val_mae: 3916.6475\n",
            "Epoch 100/1000\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 31130797.5000 - mae: 3908.3643 - val_loss: 30221804.0000 - val_mae: 4506.3286\n",
            "Epoch 101/1000\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 23824306.6667 - mae: 3777.6577 - val_loss: 26973010.0000 - val_mae: 4207.0493\n",
            "Epoch 102/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 25842376.0000 - mae: 3987.4950 - val_loss: 25466460.0000 - val_mae: 4128.2603\n",
            "Epoch 103/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 27048585.0000 - mae: 3803.5803 - val_loss: 26872356.0000 - val_mae: 4228.1543\n",
            "Epoch 104/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 21815715.1667 - mae: 3640.4439 - val_loss: 24101360.0000 - val_mae: 4004.3796\n",
            "Epoch 105/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 21868400.6667 - mae: 3522.4440 - val_loss: 27611936.0000 - val_mae: 4349.7798\n",
            "Epoch 106/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 25671273.3333 - mae: 3542.1990 - val_loss: 21292800.0000 - val_mae: 3711.9253\n",
            "Epoch 107/1000\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 22010501.0000 - mae: 3430.8141 - val_loss: 18953498.0000 - val_mae: 3509.3315\n",
            "Epoch 108/1000\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 20495990.0833 - mae: 3512.3057 - val_loss: 25180184.0000 - val_mae: 3895.5217\n",
            "Epoch 109/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 24082909.3333 - mae: 3850.6052 - val_loss: 18247564.0000 - val_mae: 3309.9656\n",
            "Epoch 110/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 14258816.5833 - mae: 2867.5538 - val_loss: 17779694.0000 - val_mae: 3329.4426\n",
            "Epoch 111/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 23565980.5833 - mae: 3499.3696 - val_loss: 17172568.0000 - val_mae: 3376.7183\n",
            "Epoch 112/1000\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 17636339.3333 - mae: 3132.8196 - val_loss: 16296205.0000 - val_mae: 3170.2327\n",
            "Epoch 113/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 14051297.6667 - mae: 2719.5010 - val_loss: 19744450.0000 - val_mae: 3691.5989\n",
            "Epoch 114/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 16886459.0000 - mae: 3076.8397 - val_loss: 15498352.0000 - val_mae: 3222.1489\n",
            "Epoch 115/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 14600187.0000 - mae: 2880.5418 - val_loss: 14022668.0000 - val_mae: 2984.2629\n",
            "Epoch 116/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 11298974.2500 - mae: 2571.7580 - val_loss: 15711780.0000 - val_mae: 3107.5564\n",
            "Epoch 117/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 19199185.1667 - mae: 2950.1051 - val_loss: 26387276.0000 - val_mae: 4333.9805\n",
            "Epoch 118/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 24545157.0000 - mae: 3440.2154 - val_loss: 17114468.0000 - val_mae: 3533.1316\n",
            "Epoch 119/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 16687475.4167 - mae: 2826.2457 - val_loss: 25560460.0000 - val_mae: 4080.4990\n",
            "Epoch 120/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 17814785.2500 - mae: 3264.0592 - val_loss: 15319315.0000 - val_mae: 3274.7515\n",
            "Epoch 121/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 14762457.2500 - mae: 3055.4841 - val_loss: 13560631.0000 - val_mae: 3098.1301\n",
            "Epoch 122/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 18189548.8333 - mae: 2638.1162 - val_loss: 13866185.0000 - val_mae: 3106.8108\n",
            "Epoch 123/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 11352378.5833 - mae: 2496.1701 - val_loss: 17148982.0000 - val_mae: 3200.3831\n",
            "Epoch 124/1000\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 10728003.7500 - mae: 2479.7024 - val_loss: 9807724.0000 - val_mae: 2502.4163\n",
            "Epoch 125/1000\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 16391461.6667 - mae: 2285.7349 - val_loss: 9437834.0000 - val_mae: 2310.6462\n",
            "Epoch 126/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7693198.7917 - mae: 1984.1630 - val_loss: 12453450.0000 - val_mae: 2605.7527\n",
            "Epoch 127/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 10735867.8750 - mae: 2196.4702 - val_loss: 8266525.0000 - val_mae: 2103.1047\n",
            "Epoch 128/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 12553515.8333 - mae: 2009.5904 - val_loss: 8835143.0000 - val_mae: 2262.1033\n",
            "Epoch 129/1000\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 9536554.3333 - mae: 1991.7145 - val_loss: 9655943.0000 - val_mae: 2272.3545\n",
            "Epoch 130/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 13616778.7500 - mae: 2192.5592 - val_loss: 8703612.0000 - val_mae: 2201.3530\n",
            "Epoch 131/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 9696189.0000 - mae: 2090.4141 - val_loss: 8551775.0000 - val_mae: 2151.9119\n",
            "Epoch 132/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 7052772.5417 - mae: 1910.1867 - val_loss: 11319176.0000 - val_mae: 2703.6680\n",
            "Epoch 133/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 10245114.6667 - mae: 2341.6140 - val_loss: 10163015.0000 - val_mae: 2378.0557\n",
            "Epoch 134/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8272171.2917 - mae: 2064.5486 - val_loss: 12554503.0000 - val_mae: 2696.3464\n",
            "Epoch 135/1000\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 18286702.0833 - mae: 2461.7358 - val_loss: 8707159.0000 - val_mae: 2357.5017\n",
            "Epoch 136/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 7293792.4167 - mae: 2023.9496 - val_loss: 7411891.0000 - val_mae: 2074.6069\n",
            "Epoch 137/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 9639229.3750 - mae: 2074.8348 - val_loss: 9696112.0000 - val_mae: 2287.2185\n",
            "Epoch 138/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 10286230.0000 - mae: 2078.5743 - val_loss: 10923083.0000 - val_mae: 2468.5952\n",
            "Epoch 139/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 11126651.0417 - mae: 2163.7613 - val_loss: 7294925.5000 - val_mae: 2075.2065\n",
            "Epoch 140/1000\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 9303752.7083 - mae: 1971.5401 - val_loss: 6953217.5000 - val_mae: 1961.9082\n",
            "Epoch 141/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 12881577.4583 - mae: 2061.2892 - val_loss: 7646305.5000 - val_mae: 2007.3529\n",
            "Epoch 142/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6833807.2083 - mae: 1885.3243 - val_loss: 7892804.0000 - val_mae: 2063.4390\n",
            "Epoch 143/1000\n",
            "11/11 [==============================] - 0s 23ms/step - loss: 12378269.2500 - mae: 2028.1875 - val_loss: 7667937.5000 - val_mae: 1966.6019\n",
            "Epoch 144/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 7529196.0000 - mae: 1955.7600 - val_loss: 9377264.0000 - val_mae: 2430.6741\n",
            "Epoch 145/1000\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 11443018.4583 - mae: 2226.0184 - val_loss: 7336726.0000 - val_mae: 1980.5813\n",
            "Epoch 146/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 8659800.8333 - mae: 2040.7498 - val_loss: 13810908.0000 - val_mae: 2886.3108\n",
            "Epoch 147/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 10153785.4167 - mae: 2113.5470 - val_loss: 6199338.0000 - val_mae: 1835.7037\n",
            "Epoch 148/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 14748606.9167 - mae: 2014.4238 - val_loss: 7184807.0000 - val_mae: 2143.9761\n",
            "Epoch 149/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6750564.7917 - mae: 1986.4153 - val_loss: 6294696.5000 - val_mae: 1817.3352\n",
            "Epoch 150/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 9141657.9167 - mae: 2022.8974 - val_loss: 8177514.0000 - val_mae: 2092.3215\n",
            "Epoch 151/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6730301.5000 - mae: 1783.6723 - val_loss: 6926957.0000 - val_mae: 1926.6370\n",
            "Epoch 152/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 16257464.3333 - mae: 2146.1977 - val_loss: 7681370.5000 - val_mae: 2189.0530\n",
            "Epoch 153/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 7852898.7083 - mae: 1893.0067 - val_loss: 7473943.5000 - val_mae: 2184.9663\n",
            "Epoch 154/1000\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 8295522.5000 - mae: 1903.2765 - val_loss: 6470243.0000 - val_mae: 1916.5636\n",
            "Epoch 155/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 15087427.0000 - mae: 2077.6984 - val_loss: 13410439.0000 - val_mae: 3081.5513\n",
            "Epoch 156/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 11420315.3333 - mae: 2450.4076 - val_loss: 7364571.0000 - val_mae: 2112.2725\n",
            "Epoch 157/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 10372651.7083 - mae: 1853.7348 - val_loss: 6136406.5000 - val_mae: 1876.2516\n",
            "Epoch 158/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 11490183.4167 - mae: 1840.6879 - val_loss: 7311980.0000 - val_mae: 2030.9664\n",
            "Epoch 159/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 8155282.9583 - mae: 1815.1419 - val_loss: 6067316.5000 - val_mae: 1857.9297\n",
            "Epoch 160/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 6822382.3750 - mae: 1777.7407 - val_loss: 8359227.0000 - val_mae: 2098.3364\n",
            "Epoch 161/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5785802.3333 - mae: 1742.1225 - val_loss: 6500453.5000 - val_mae: 1855.8824\n",
            "Epoch 162/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 6126310.2083 - mae: 1702.2035 - val_loss: 5592057.0000 - val_mae: 1754.0717\n",
            "Epoch 163/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 4639533.7917 - mae: 1573.9713 - val_loss: 5547918.0000 - val_mae: 1796.9962\n",
            "Epoch 164/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7408803.6667 - mae: 1905.5590 - val_loss: 5271042.5000 - val_mae: 1707.7511\n",
            "Epoch 165/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 6060704.1458 - mae: 1668.2374 - val_loss: 12522904.0000 - val_mae: 2902.1641\n",
            "Epoch 166/1000\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 16757984.8333 - mae: 2171.3362 - val_loss: 5474625.0000 - val_mae: 1712.6243\n",
            "Epoch 167/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 8903174.6458 - mae: 1611.3203 - val_loss: 5400905.0000 - val_mae: 1733.5144\n",
            "Epoch 168/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 6212073.8125 - mae: 1615.4633 - val_loss: 5847451.5000 - val_mae: 1884.6860\n",
            "Epoch 169/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6466617.2917 - mae: 1756.8145 - val_loss: 8600320.0000 - val_mae: 2298.1768\n",
            "Epoch 170/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 9043925.9583 - mae: 1844.2286 - val_loss: 7517260.0000 - val_mae: 2076.2920\n",
            "Epoch 171/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6143514.1042 - mae: 1598.8697 - val_loss: 6508001.0000 - val_mae: 1910.9977\n",
            "Epoch 172/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5315609.7917 - mae: 1572.9449 - val_loss: 5487202.5000 - val_mae: 1727.4193\n",
            "Epoch 173/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4540213.9792 - mae: 1512.2366 - val_loss: 5762358.5000 - val_mae: 1881.8591\n",
            "Epoch 174/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5737557.5000 - mae: 1652.0137 - val_loss: 5128387.5000 - val_mae: 1725.3175\n",
            "Epoch 175/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5734117.6458 - mae: 1540.3406 - val_loss: 5046139.0000 - val_mae: 1665.0022\n",
            "Epoch 176/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7497296.2500 - mae: 1524.9641 - val_loss: 5099273.0000 - val_mae: 1689.5459\n",
            "Epoch 177/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5537065.5000 - mae: 1728.3254 - val_loss: 5271686.5000 - val_mae: 1761.2067\n",
            "Epoch 178/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6969922.1667 - mae: 1774.7067 - val_loss: 5575790.0000 - val_mae: 1736.9589\n",
            "Epoch 179/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5896485.9167 - mae: 1794.2492 - val_loss: 6079281.0000 - val_mae: 1999.4218\n",
            "Epoch 180/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5698504.2083 - mae: 1794.9930 - val_loss: 5251170.0000 - val_mae: 1701.0610\n",
            "Epoch 181/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6402847.4375 - mae: 1588.1800 - val_loss: 5486538.5000 - val_mae: 1701.8835\n",
            "Epoch 182/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5687308.0833 - mae: 1562.4934 - val_loss: 15367877.0000 - val_mae: 3292.8337\n",
            "Epoch 183/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 12656652.3333 - mae: 2440.2033 - val_loss: 5033681.0000 - val_mae: 1729.3400\n",
            "Epoch 184/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8279663.4167 - mae: 1819.3617 - val_loss: 5162374.5000 - val_mae: 1777.1821\n",
            "Epoch 185/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 9180916.0417 - mae: 1926.3919 - val_loss: 9398708.0000 - val_mae: 2433.3147\n",
            "Epoch 186/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6309904.7500 - mae: 1778.0922 - val_loss: 8224010.5000 - val_mae: 2278.3894\n",
            "Epoch 187/1000\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 8746084.6667 - mae: 1879.8850 - val_loss: 5475738.5000 - val_mae: 1744.5820\n",
            "Epoch 188/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4837486.8958 - mae: 1450.1802 - val_loss: 6061088.0000 - val_mae: 1880.5967\n",
            "Epoch 189/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 14764951.5833 - mae: 1961.4199 - val_loss: 5784620.0000 - val_mae: 1958.8190\n",
            "Epoch 190/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6264423.9583 - mae: 1603.4125 - val_loss: 4492562.0000 - val_mae: 1624.7836\n",
            "Epoch 191/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4445336.1667 - mae: 1552.7689 - val_loss: 4801064.5000 - val_mae: 1635.4183\n",
            "Epoch 192/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6318702.8542 - mae: 1697.5563 - val_loss: 8924728.0000 - val_mae: 2522.1194\n",
            "Epoch 193/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 10758801.2500 - mae: 1845.4728 - val_loss: 5089130.5000 - val_mae: 1695.8773\n",
            "Epoch 194/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 13562305.1667 - mae: 1774.6490 - val_loss: 4891124.5000 - val_mae: 1653.5092\n",
            "Epoch 195/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 6602185.3958 - mae: 1513.0803 - val_loss: 6470537.5000 - val_mae: 1984.0267\n",
            "Epoch 196/1000\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 4951269.5000 - mae: 1553.5463 - val_loss: 4539817.5000 - val_mae: 1590.4568\n",
            "Epoch 197/1000\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 8386411.2917 - mae: 1540.0913 - val_loss: 5033354.0000 - val_mae: 1781.5519\n",
            "Epoch 198/1000\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 4978548.7083 - mae: 1566.1262 - val_loss: 4992624.0000 - val_mae: 1704.6023\n",
            "Epoch 199/1000\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 4741152.1042 - mae: 1581.2703 - val_loss: 8218733.5000 - val_mae: 2432.8254\n",
            "Epoch 200/1000\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 10770619.2083 - mae: 2179.6636 - val_loss: 5676434.5000 - val_mae: 1844.3890\n",
            "Epoch 201/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 9262344.6250 - mae: 1770.3012 - val_loss: 4896800.5000 - val_mae: 1675.2533\n",
            "Epoch 202/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7300901.3542 - mae: 1577.1241 - val_loss: 5294560.5000 - val_mae: 1761.2305\n",
            "Epoch 203/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4626069.2500 - mae: 1541.4586 - val_loss: 4969018.5000 - val_mae: 1801.7222\n",
            "Epoch 204/1000\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 7045887.7708 - mae: 1630.7326 - val_loss: 4271571.0000 - val_mae: 1546.7391\n",
            "Epoch 205/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4269735.4583 - mae: 1491.4834 - val_loss: 7168473.5000 - val_mae: 2142.5754\n",
            "Epoch 206/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 10574500.1458 - mae: 2061.5274 - val_loss: 4797050.0000 - val_mae: 1658.3728\n",
            "Epoch 207/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 8731583.9375 - mae: 1804.7921 - val_loss: 5112897.0000 - val_mae: 1730.9232\n",
            "Epoch 208/1000\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 10651211.2083 - mae: 1689.1625 - val_loss: 5349709.5000 - val_mae: 1713.2970\n",
            "Epoch 209/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 7609401.2083 - mae: 1659.4672 - val_loss: 4939732.5000 - val_mae: 1704.5757\n",
            "Epoch 210/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 12848212.7917 - mae: 1556.5553 - val_loss: 4670674.0000 - val_mae: 1685.4089\n",
            "Epoch 211/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6921752.7917 - mae: 1614.5665 - val_loss: 5538040.0000 - val_mae: 1831.5662\n",
            "Epoch 212/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 10996799.6667 - mae: 1820.3149 - val_loss: 4762704.5000 - val_mae: 1753.2753\n",
            "Epoch 213/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5156422.2917 - mae: 1478.9557 - val_loss: 6066742.0000 - val_mae: 1952.8293\n",
            "Epoch 214/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5943331.3958 - mae: 1676.3762 - val_loss: 4315724.0000 - val_mae: 1577.8171\n",
            "Epoch 215/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 6389258.6250 - mae: 1491.3410 - val_loss: 6733954.0000 - val_mae: 2078.9290\n",
            "Epoch 216/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 12661436.1667 - mae: 2156.6356 - val_loss: 4641955.5000 - val_mae: 1698.8529\n",
            "Epoch 217/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 15488132.6667 - mae: 2232.0771 - val_loss: 4156259.5000 - val_mae: 1535.2587\n",
            "Epoch 218/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4213919.0417 - mae: 1387.2332 - val_loss: 4347092.5000 - val_mae: 1580.0308\n",
            "Epoch 219/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3976880.8125 - mae: 1421.8005 - val_loss: 4705770.0000 - val_mae: 1740.2933\n",
            "Epoch 220/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 7649326.3958 - mae: 1592.5394 - val_loss: 7499382.5000 - val_mae: 2249.4668\n",
            "Epoch 221/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 7608042.5417 - mae: 1845.4533 - val_loss: 5667847.0000 - val_mae: 1869.5050\n",
            "Epoch 222/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 8132850.8333 - mae: 1655.4905 - val_loss: 5192987.5000 - val_mae: 1768.3376\n",
            "Epoch 223/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 4639663.2292 - mae: 1583.3488 - val_loss: 5501010.0000 - val_mae: 1914.1703\n",
            "Epoch 224/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 9387978.6250 - mae: 1783.3531 - val_loss: 4094161.5000 - val_mae: 1530.8634\n",
            "Epoch 225/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7517656.6667 - mae: 1636.1452 - val_loss: 6635165.0000 - val_mae: 1994.5704\n",
            "Epoch 226/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6018074.9375 - mae: 1834.3478 - val_loss: 6258549.0000 - val_mae: 2015.5817\n",
            "Epoch 227/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4922187.0417 - mae: 1545.0479 - val_loss: 4197240.0000 - val_mae: 1583.4144\n",
            "Epoch 228/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4994751.3333 - mae: 1497.3135 - val_loss: 5469642.0000 - val_mae: 1846.1494\n",
            "Epoch 229/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 3736607.0625 - mae: 1420.3038 - val_loss: 8266104.0000 - val_mae: 2396.6199\n",
            "Epoch 230/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 9357276.2500 - mae: 2106.0181 - val_loss: 4579329.5000 - val_mae: 1632.6168\n",
            "Epoch 231/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 9033066.5417 - mae: 1940.9053 - val_loss: 6971056.0000 - val_mae: 2078.6980\n",
            "Epoch 232/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6502783.7500 - mae: 1839.6813 - val_loss: 3768017.0000 - val_mae: 1525.0447\n",
            "Epoch 233/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5472349.5000 - mae: 1511.3520 - val_loss: 4073590.5000 - val_mae: 1545.6165\n",
            "Epoch 234/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6542281.3750 - mae: 1544.5632 - val_loss: 4237698.0000 - val_mae: 1568.2048\n",
            "Epoch 235/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 8877612.4167 - mae: 1692.8405 - val_loss: 6655579.5000 - val_mae: 2053.0635\n",
            "Epoch 236/1000\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 13342583.1250 - mae: 1686.8203 - val_loss: 4522502.5000 - val_mae: 1686.8871\n",
            "Epoch 237/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 12955613.3333 - mae: 1806.3726 - val_loss: 6238600.0000 - val_mae: 2031.9606\n",
            "Epoch 238/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5742026.7917 - mae: 1796.3491 - val_loss: 4354968.5000 - val_mae: 1603.4487\n",
            "Epoch 239/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 9644027.3125 - mae: 1850.4528 - val_loss: 5239063.5000 - val_mae: 1781.5045\n",
            "Epoch 240/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5522511.8542 - mae: 1728.8333 - val_loss: 9518577.0000 - val_mae: 2577.9373\n",
            "Epoch 241/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7729892.0833 - mae: 1934.8844 - val_loss: 4973561.5000 - val_mae: 1769.6597\n",
            "Epoch 242/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 12603758.3750 - mae: 1607.5105 - val_loss: 5449108.5000 - val_mae: 1844.3279\n",
            "Epoch 243/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8370732.8542 - mae: 1887.8150 - val_loss: 3792602.2500 - val_mae: 1548.2120\n",
            "Epoch 244/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6897640.7500 - mae: 1736.9961 - val_loss: 3718079.7500 - val_mae: 1468.2885\n",
            "Epoch 245/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3504041.3542 - mae: 1365.6879 - val_loss: 4009514.5000 - val_mae: 1524.4398\n",
            "Epoch 246/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 13162559.1250 - mae: 1744.5981 - val_loss: 6192480.5000 - val_mae: 2026.5715\n",
            "Epoch 247/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6280672.5417 - mae: 1852.3932 - val_loss: 11882027.0000 - val_mae: 2968.3843\n",
            "Epoch 248/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8002929.7500 - mae: 2255.3479 - val_loss: 4949936.5000 - val_mae: 1742.3942\n",
            "Epoch 249/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5070160.9375 - mae: 1395.1870 - val_loss: 3726141.7500 - val_mae: 1472.5404\n",
            "Epoch 250/1000\n",
            "11/11 [==============================] - 0s 23ms/step - loss: 4678265.2292 - mae: 1431.3831 - val_loss: 3793931.0000 - val_mae: 1489.2891\n",
            "Epoch 251/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4885071.3542 - mae: 1386.3209 - val_loss: 4047581.2500 - val_mae: 1547.8967\n",
            "Epoch 252/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 9467912.3542 - mae: 1581.4012 - val_loss: 4908965.5000 - val_mae: 1699.3960\n",
            "Epoch 253/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4864497.5417 - mae: 1515.9723 - val_loss: 3799759.5000 - val_mae: 1524.7847\n",
            "Epoch 254/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5798017.9583 - mae: 1694.4705 - val_loss: 7550447.5000 - val_mae: 2220.7686\n",
            "Epoch 255/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6455549.7083 - mae: 1766.0247 - val_loss: 4263735.0000 - val_mae: 1553.2896\n",
            "Epoch 256/1000\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 9749229.2708 - mae: 1645.1851 - val_loss: 4538245.0000 - val_mae: 1704.6005\n",
            "Epoch 257/1000\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 8119728.6875 - mae: 1670.3377 - val_loss: 3865947.0000 - val_mae: 1502.0929\n",
            "Epoch 258/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 12230497.5000 - mae: 1563.6423 - val_loss: 3742215.5000 - val_mae: 1507.5676\n",
            "Epoch 259/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5289237.5833 - mae: 1378.3855 - val_loss: 3644230.0000 - val_mae: 1491.4592\n",
            "Epoch 260/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5019402.0417 - mae: 1433.4169 - val_loss: 3839511.7500 - val_mae: 1518.8342\n",
            "Epoch 261/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4152513.1875 - mae: 1306.5961 - val_loss: 3583331.2500 - val_mae: 1462.2537\n",
            "Epoch 262/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 3252655.3542 - mae: 1316.8716 - val_loss: 5820372.5000 - val_mae: 1956.7294\n",
            "Epoch 263/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5215333.2708 - mae: 1577.2140 - val_loss: 3527937.7500 - val_mae: 1484.8832\n",
            "Epoch 264/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 4679923.8750 - mae: 1339.0887 - val_loss: 3872854.5000 - val_mae: 1518.6641\n",
            "Epoch 265/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 7576618.5833 - mae: 1428.1396 - val_loss: 3694655.7500 - val_mae: 1495.9967\n",
            "Epoch 266/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3340607.1458 - mae: 1261.7164 - val_loss: 3394618.7500 - val_mae: 1410.8708\n",
            "Epoch 267/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 11912450.3750 - mae: 1485.3137 - val_loss: 3641374.0000 - val_mae: 1476.5293\n",
            "Epoch 268/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6821400.1458 - mae: 1480.8643 - val_loss: 6385061.5000 - val_mae: 2077.9744\n",
            "Epoch 269/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 7351004.0833 - mae: 1613.6514 - val_loss: 3688052.5000 - val_mae: 1480.2883\n",
            "Epoch 270/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8201763.1250 - mae: 1531.5431 - val_loss: 3416545.0000 - val_mae: 1417.0861\n",
            "Epoch 271/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3841771.2292 - mae: 1238.7485 - val_loss: 4786787.5000 - val_mae: 1764.0945\n",
            "Epoch 272/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4504326.7500 - mae: 1405.9170 - val_loss: 4053654.2500 - val_mae: 1593.9331\n",
            "Epoch 273/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5891772.4583 - mae: 1411.8387 - val_loss: 3927047.5000 - val_mae: 1535.2363\n",
            "Epoch 274/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 3357625.1458 - mae: 1330.3518 - val_loss: 4533642.5000 - val_mae: 1692.5674\n",
            "Epoch 275/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 13602715.7917 - mae: 1956.4635 - val_loss: 5080243.0000 - val_mae: 1756.4698\n",
            "Epoch 276/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 4236353.6250 - mae: 1435.0964 - val_loss: 4266685.0000 - val_mae: 1622.3942\n",
            "Epoch 277/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 9677660.1042 - mae: 1612.9799 - val_loss: 4007002.0000 - val_mae: 1527.9218\n",
            "Epoch 278/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5530382.3125 - mae: 1298.8719 - val_loss: 3180752.7500 - val_mae: 1374.3706\n",
            "Epoch 279/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 8951463.8750 - mae: 1482.4142 - val_loss: 4698704.5000 - val_mae: 1731.0693\n",
            "Epoch 280/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 8539360.8333 - mae: 1613.9595 - val_loss: 4079430.2500 - val_mae: 1587.0460\n",
            "Epoch 281/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 9603182.5208 - mae: 1603.7756 - val_loss: 4086926.7500 - val_mae: 1564.1597\n",
            "Epoch 282/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3500999.9375 - mae: 1314.1955 - val_loss: 3281850.7500 - val_mae: 1417.6283\n",
            "Epoch 283/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5780527.7917 - mae: 1372.6264 - val_loss: 4793427.0000 - val_mae: 1763.8727\n",
            "Epoch 284/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6157566.2500 - mae: 1515.4355 - val_loss: 3505742.5000 - val_mae: 1457.8192\n",
            "Epoch 285/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 13374421.8333 - mae: 1839.3196 - val_loss: 3887346.0000 - val_mae: 1512.7286\n",
            "Epoch 286/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 11573921.2917 - mae: 1472.2688 - val_loss: 3866609.5000 - val_mae: 1501.4409\n",
            "Epoch 287/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 9649504.6042 - mae: 1607.4722 - val_loss: 6330491.0000 - val_mae: 2046.2828\n",
            "Epoch 288/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5171018.1458 - mae: 1712.7290 - val_loss: 5674126.5000 - val_mae: 1919.9873\n",
            "Epoch 289/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3831628.4167 - mae: 1396.3665 - val_loss: 3291067.5000 - val_mae: 1384.1488\n",
            "Epoch 290/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3697585.9479 - mae: 1216.8070 - val_loss: 4809874.5000 - val_mae: 1781.6759\n",
            "Epoch 291/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6680783.0625 - mae: 1438.4091 - val_loss: 4453742.5000 - val_mae: 1667.8204\n",
            "Epoch 292/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 3696952.6667 - mae: 1282.4879 - val_loss: 3565106.7500 - val_mae: 1482.2655\n",
            "Epoch 293/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 11820749.7917 - mae: 1469.5729 - val_loss: 3724675.0000 - val_mae: 1505.1561\n",
            "Epoch 294/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5825636.4167 - mae: 1372.2344 - val_loss: 4598829.0000 - val_mae: 1683.7142\n",
            "Epoch 295/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6722896.9167 - mae: 1443.0810 - val_loss: 3351734.0000 - val_mae: 1393.6730\n",
            "Epoch 296/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5440824.5625 - mae: 1335.4487 - val_loss: 4553308.0000 - val_mae: 1743.9194\n",
            "Epoch 297/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3376439.8958 - mae: 1379.3314 - val_loss: 6750887.0000 - val_mae: 2132.7317\n",
            "Epoch 298/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 6330641.7500 - mae: 1809.7173 - val_loss: 7455157.5000 - val_mae: 2305.8765\n",
            "Epoch 299/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6160123.9167 - mae: 1811.2744 - val_loss: 3240535.2500 - val_mae: 1398.3535\n",
            "Epoch 300/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 7251622.3542 - mae: 1440.7853 - val_loss: 3766344.5000 - val_mae: 1497.7800\n",
            "Epoch 301/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4175802.1042 - mae: 1281.1116 - val_loss: 3198848.7500 - val_mae: 1390.6394\n",
            "Epoch 302/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 5451044.7188 - mae: 1454.6318 - val_loss: 3416677.2500 - val_mae: 1400.3950\n",
            "Epoch 303/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 9685136.1875 - mae: 1576.4358 - val_loss: 3288416.5000 - val_mae: 1409.7228\n",
            "Epoch 304/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5155021.1458 - mae: 1375.2987 - val_loss: 3330062.0000 - val_mae: 1371.1630\n",
            "Epoch 305/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6678554.8333 - mae: 1492.5732 - val_loss: 3213928.2500 - val_mae: 1387.1196\n",
            "Epoch 306/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 3765304.5417 - mae: 1215.0476 - val_loss: 3409167.5000 - val_mae: 1414.1008\n",
            "Epoch 307/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4001099.0000 - mae: 1243.4998 - val_loss: 6105470.0000 - val_mae: 2069.8718\n",
            "Epoch 308/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6703131.2292 - mae: 1610.9321 - val_loss: 5480265.0000 - val_mae: 1828.7527\n",
            "Epoch 309/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5605553.9583 - mae: 1775.7624 - val_loss: 4834461.0000 - val_mae: 1788.4514\n",
            "Epoch 310/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5847815.3750 - mae: 1541.2557 - val_loss: 3193857.7500 - val_mae: 1348.7589\n",
            "Epoch 311/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5820266.3333 - mae: 1432.9442 - val_loss: 6633120.5000 - val_mae: 2170.5864\n",
            "Epoch 312/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 10800441.0833 - mae: 1896.7792 - val_loss: 5931296.5000 - val_mae: 1956.2365\n",
            "Epoch 313/1000\n",
            "11/11 [==============================] - 0s 25ms/step - loss: 8072050.0625 - mae: 1730.2733 - val_loss: 8715597.0000 - val_mae: 2545.1025\n",
            "Epoch 314/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 14486088.8333 - mae: 1990.6005 - val_loss: 5738815.0000 - val_mae: 1647.6765\n",
            "Epoch 315/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4378200.9583 - mae: 1309.4710 - val_loss: 3437156.2500 - val_mae: 1383.6670\n",
            "Epoch 316/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4801052.8333 - mae: 1271.7559 - val_loss: 3595704.2500 - val_mae: 1480.9297\n",
            "Epoch 317/1000\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 3377708.3646 - mae: 1199.7295 - val_loss: 3548889.5000 - val_mae: 1402.7402\n",
            "Epoch 318/1000\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 6727332.0833 - mae: 1505.9490 - val_loss: 4384676.5000 - val_mae: 1629.3446\n",
            "Epoch 319/1000\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 6158429.3333 - mae: 1488.5749 - val_loss: 3308516.2500 - val_mae: 1341.1073\n",
            "Epoch 320/1000\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 3972536.0833 - mae: 1490.9678 - val_loss: 4409276.0000 - val_mae: 1560.5474\n",
            "Epoch 321/1000\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 4640375.9375 - mae: 1607.1044 - val_loss: 3430214.2500 - val_mae: 1400.6304\n",
            "Epoch 322/1000\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 9383820.1667 - mae: 1551.0324 - val_loss: 3509552.7500 - val_mae: 1422.2794\n",
            "Epoch 323/1000\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 4271806.3958 - mae: 1330.2205 - val_loss: 5344623.5000 - val_mae: 1895.1422\n",
            "Epoch 324/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3961644.8958 - mae: 1323.2474 - val_loss: 3548022.0000 - val_mae: 1414.7909\n",
            "Epoch 325/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3579388.6979 - mae: 1169.7945 - val_loss: 3495597.2500 - val_mae: 1421.8165\n",
            "Epoch 326/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3886067.1667 - mae: 1314.7247 - val_loss: 3912469.0000 - val_mae: 1541.0948\n",
            "Epoch 327/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4490870.7083 - mae: 1533.4976 - val_loss: 4205621.5000 - val_mae: 1457.4543\n",
            "Epoch 328/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 7354906.9167 - mae: 1409.0060 - val_loss: 4414479.0000 - val_mae: 1604.7881\n",
            "Epoch 329/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3360945.1979 - mae: 1237.7872 - val_loss: 4312367.5000 - val_mae: 1663.8665\n",
            "Epoch 330/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3795178.7083 - mae: 1374.9955 - val_loss: 5109996.0000 - val_mae: 1547.5675\n",
            "Epoch 331/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3954473.7292 - mae: 1316.4998 - val_loss: 3790335.0000 - val_mae: 1458.0701\n",
            "Epoch 332/1000\n",
            "11/11 [==============================] - 0s 14ms/step - loss: 5733414.6562 - mae: 1294.3314 - val_loss: 3926202.0000 - val_mae: 1474.5701\n",
            "Epoch 333/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4850948.0000 - mae: 1300.6943 - val_loss: 8453670.0000 - val_mae: 1628.7006\n",
            "Epoch 334/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6092515.1875 - mae: 1313.1019 - val_loss: 4233004.5000 - val_mae: 1589.5267\n",
            "Epoch 335/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 7862565.0625 - mae: 1531.6723 - val_loss: 3447086.5000 - val_mae: 1393.7408\n",
            "Epoch 336/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 7467218.2083 - mae: 1590.4585 - val_loss: 7935608.0000 - val_mae: 2341.4155\n",
            "Epoch 337/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 11622940.5833 - mae: 2078.0068 - val_loss: 4972972.5000 - val_mae: 1767.8075\n",
            "Epoch 338/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4360815.3333 - mae: 1379.8773 - val_loss: 4504230.5000 - val_mae: 1718.0923\n",
            "Epoch 339/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3989758.1667 - mae: 1329.9185 - val_loss: 3866810.5000 - val_mae: 1446.9095\n",
            "Epoch 340/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5253716.6875 - mae: 1586.5210 - val_loss: 10158301.0000 - val_mae: 2720.3477\n",
            "Epoch 341/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6806696.9583 - mae: 2039.6543 - val_loss: 13134061.0000 - val_mae: 3196.8984\n",
            "Epoch 342/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 8088310.1250 - mae: 2195.8641 - val_loss: 4586990.0000 - val_mae: 1719.5791\n",
            "Epoch 343/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4814084.6667 - mae: 1589.6583 - val_loss: 6103180.5000 - val_mae: 2004.3329\n",
            "Epoch 344/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 9993074.9583 - mae: 1790.5520 - val_loss: 4383589.0000 - val_mae: 1636.3623\n",
            "Epoch 345/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6018038.8750 - mae: 1407.0094 - val_loss: 3697554.7500 - val_mae: 1459.2236\n",
            "Epoch 346/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 11127643.2500 - mae: 1453.9273 - val_loss: 3718002.0000 - val_mae: 1484.3152\n",
            "Epoch 347/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3223857.5938 - mae: 1299.5204 - val_loss: 3780695.0000 - val_mae: 1480.8505\n",
            "Epoch 348/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 11326762.4583 - mae: 1556.0755 - val_loss: 4038221.2500 - val_mae: 1581.1272\n",
            "Epoch 349/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 12183477.2083 - mae: 1566.9994 - val_loss: 5440166.5000 - val_mae: 1874.3302\n",
            "Epoch 350/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 9188806.8333 - mae: 1731.9639 - val_loss: 3829245.5000 - val_mae: 1484.6646\n",
            "Epoch 351/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7645245.0000 - mae: 1431.4879 - val_loss: 3786441.2500 - val_mae: 1527.0540\n",
            "Epoch 352/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6035778.7708 - mae: 1598.1995 - val_loss: 3761128.2500 - val_mae: 1433.5242\n",
            "Epoch 353/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4209242.1458 - mae: 1445.0241 - val_loss: 5250153.0000 - val_mae: 1851.1119\n",
            "Epoch 354/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6331294.7917 - mae: 1394.7782 - val_loss: 3278085.2500 - val_mae: 1364.7682\n",
            "Epoch 355/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8005840.4167 - mae: 1353.2355 - val_loss: 3335572.7500 - val_mae: 1414.4186\n",
            "Epoch 356/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4582812.5625 - mae: 1357.8838 - val_loss: 3559143.5000 - val_mae: 1397.4800\n",
            "Epoch 357/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 11913501.5833 - mae: 1683.2015 - val_loss: 3303550.5000 - val_mae: 1393.5342\n",
            "Epoch 358/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 8965789.6250 - mae: 1469.9157 - val_loss: 3776526.5000 - val_mae: 1499.2290\n",
            "Epoch 359/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4114569.1146 - mae: 1209.2103 - val_loss: 3245558.7500 - val_mae: 1325.7489\n",
            "Epoch 360/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6172325.7708 - mae: 1503.4926 - val_loss: 3465566.7500 - val_mae: 1403.6481\n",
            "Epoch 361/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 10921697.6667 - mae: 1418.9012 - val_loss: 3662795.5000 - val_mae: 1456.4945\n",
            "Epoch 362/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5824156.6042 - mae: 1264.9898 - val_loss: 4736415.0000 - val_mae: 1741.1661\n",
            "Epoch 363/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5501527.5208 - mae: 1431.8702 - val_loss: 7732357.0000 - val_mae: 2139.5352\n",
            "Epoch 364/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5467448.0833 - mae: 1754.9159 - val_loss: 3759085.5000 - val_mae: 1420.2352\n",
            "Epoch 365/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 7930998.2292 - mae: 1298.7469 - val_loss: 3762199.0000 - val_mae: 1443.5195\n",
            "Epoch 366/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5246452.4167 - mae: 1240.5027 - val_loss: 3848535.0000 - val_mae: 1379.3372\n",
            "Epoch 367/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3306635.7708 - mae: 1217.7044 - val_loss: 5618638.5000 - val_mae: 1961.0975\n",
            "Epoch 368/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 7508246.7083 - mae: 1582.2789 - val_loss: 6515885.0000 - val_mae: 1767.0264\n",
            "Epoch 369/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 7890325.0208 - mae: 1443.5709 - val_loss: 2975348.5000 - val_mae: 1314.6450\n",
            "Epoch 370/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3951196.2500 - mae: 1258.8970 - val_loss: 3522513.5000 - val_mae: 1449.2003\n",
            "Epoch 371/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4398588.2604 - mae: 1411.0682 - val_loss: 4003952.5000 - val_mae: 1530.0587\n",
            "Epoch 372/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6465297.0208 - mae: 1290.7241 - val_loss: 3740082.0000 - val_mae: 1473.8961\n",
            "Epoch 373/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4532461.1875 - mae: 1312.1078 - val_loss: 3296916.2500 - val_mae: 1337.9476\n",
            "Epoch 374/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 10874466.4167 - mae: 1478.3943 - val_loss: 7964798.5000 - val_mae: 2416.2651\n",
            "Epoch 375/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6674998.8333 - mae: 1612.3491 - val_loss: 3764084.2500 - val_mae: 1365.8661\n",
            "Epoch 376/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 7756831.5833 - mae: 1332.4159 - val_loss: 3782601.5000 - val_mae: 1425.3290\n",
            "Epoch 377/1000\n",
            "11/11 [==============================] - 0s 23ms/step - loss: 3071629.2188 - mae: 1123.9030 - val_loss: 5783141.0000 - val_mae: 1989.4390\n",
            "Epoch 378/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4602078.2917 - mae: 1585.2669 - val_loss: 5374767.0000 - val_mae: 1714.4451\n",
            "Epoch 379/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8043771.6667 - mae: 1999.3263 - val_loss: 12664175.0000 - val_mae: 3174.2651\n",
            "Epoch 380/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 9421146.8333 - mae: 2359.4213 - val_loss: 9277962.0000 - val_mae: 2198.9302\n",
            "Epoch 381/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6080699.4583 - mae: 1775.3512 - val_loss: 6676382.5000 - val_mae: 2187.8228\n",
            "Epoch 382/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5068800.6042 - mae: 1580.4863 - val_loss: 3252752.2500 - val_mae: 1314.7723\n",
            "Epoch 383/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3051241.5833 - mae: 1268.5359 - val_loss: 5058445.5000 - val_mae: 1730.7706\n",
            "Epoch 384/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6315516.9167 - mae: 1477.6971 - val_loss: 4317767.5000 - val_mae: 1608.3751\n",
            "Epoch 385/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4963143.3646 - mae: 1229.2636 - val_loss: 3925390.0000 - val_mae: 1415.5690\n",
            "Epoch 386/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4336813.0000 - mae: 1203.2369 - val_loss: 4672185.5000 - val_mae: 1658.8235\n",
            "Epoch 387/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8434950.0208 - mae: 1703.8806 - val_loss: 4858202.5000 - val_mae: 1546.8859\n",
            "Epoch 388/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6540709.0104 - mae: 1416.2236 - val_loss: 3395963.5000 - val_mae: 1317.2917\n",
            "Epoch 389/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5329599.1458 - mae: 1198.1664 - val_loss: 4952419.0000 - val_mae: 1499.7782\n",
            "Epoch 390/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3239791.8958 - mae: 1206.1378 - val_loss: 4814526.5000 - val_mae: 1422.1172\n",
            "Epoch 391/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4061264.1042 - mae: 1176.5116 - val_loss: 3701307.2500 - val_mae: 1358.3909\n",
            "Epoch 392/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3107856.4167 - mae: 1151.4944 - val_loss: 6179632.0000 - val_mae: 1823.5392\n",
            "Epoch 393/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 7601157.8750 - mae: 1729.0973 - val_loss: 4402170.0000 - val_mae: 1619.2067\n",
            "Epoch 394/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 9422583.0833 - mae: 1664.2950 - val_loss: 6664750.0000 - val_mae: 2068.9033\n",
            "Epoch 395/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 11905669.6250 - mae: 1868.3779 - val_loss: 7578032.0000 - val_mae: 2243.4314\n",
            "Epoch 396/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8577144.3958 - mae: 1915.8020 - val_loss: 6756568.0000 - val_mae: 2066.4885\n",
            "Epoch 397/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 8917885.9167 - mae: 1732.0071 - val_loss: 4898987.0000 - val_mae: 1653.1718\n",
            "Epoch 398/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6484115.6875 - mae: 1402.3290 - val_loss: 4884739.5000 - val_mae: 1528.9283\n",
            "Epoch 399/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5984179.2708 - mae: 1395.4042 - val_loss: 5072801.0000 - val_mae: 1800.6637\n",
            "Epoch 400/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5127058.4583 - mae: 1521.6509 - val_loss: 8599456.0000 - val_mae: 2116.2329\n",
            "Epoch 401/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5356221.8125 - mae: 1726.0756 - val_loss: 5937101.0000 - val_mae: 1985.4017\n",
            "Epoch 402/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6127435.7500 - mae: 1700.4089 - val_loss: 6287002.5000 - val_mae: 1708.3728\n",
            "Epoch 403/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4318245.8958 - mae: 1477.5185 - val_loss: 9735773.0000 - val_mae: 2447.9951\n",
            "Epoch 404/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7448382.1250 - mae: 1732.7330 - val_loss: 7146272.0000 - val_mae: 1546.3336\n",
            "Epoch 405/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 8077128.5625 - mae: 1259.1780 - val_loss: 5284742.5000 - val_mae: 1550.0336\n",
            "Epoch 406/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 11322267.6667 - mae: 1385.5333 - val_loss: 8294431.0000 - val_mae: 1427.4419\n",
            "Epoch 407/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3423492.6667 - mae: 1239.3358 - val_loss: 6018105.5000 - val_mae: 1457.3165\n",
            "Epoch 408/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 4034135.1250 - mae: 1269.5918 - val_loss: 9017615.0000 - val_mae: 1715.0498\n",
            "Epoch 409/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4515176.2500 - mae: 1267.8018 - val_loss: 7393918.0000 - val_mae: 1455.0776\n",
            "Epoch 410/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3296603.6562 - mae: 1363.3579 - val_loss: 11235047.0000 - val_mae: 1524.1633\n",
            "Epoch 411/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 12619812.6667 - mae: 2270.0132 - val_loss: 11563341.0000 - val_mae: 2749.8818\n",
            "Epoch 412/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 10179655.9167 - mae: 2594.5447 - val_loss: 3719575.7500 - val_mae: 1384.9901\n",
            "Epoch 413/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4727387.0208 - mae: 1437.7339 - val_loss: 3731317.7500 - val_mae: 1427.5699\n",
            "Epoch 414/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5158258.0417 - mae: 1359.7742 - val_loss: 4067678.0000 - val_mae: 1518.2366\n",
            "Epoch 415/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6998519.4375 - mae: 1451.4945 - val_loss: 4552306.5000 - val_mae: 1669.7446\n",
            "Epoch 416/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3881449.9375 - mae: 1290.5034 - val_loss: 3245248.0000 - val_mae: 1374.4991\n",
            "Epoch 417/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7154016.9896 - mae: 1287.8562 - val_loss: 3147275.7500 - val_mae: 1315.3347\n",
            "Epoch 418/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5329531.2083 - mae: 1355.6818 - val_loss: 3259366.7500 - val_mae: 1363.4728\n",
            "Epoch 419/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3557861.0000 - mae: 1274.2088 - val_loss: 3351004.5000 - val_mae: 1350.4210\n",
            "Epoch 420/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 2923416.6875 - mae: 1202.4701 - val_loss: 3149909.7500 - val_mae: 1338.4286\n",
            "Epoch 421/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8441843.2292 - mae: 1335.4927 - val_loss: 3336858.2500 - val_mae: 1398.9794\n",
            "Epoch 422/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3061334.1458 - mae: 1150.2145 - val_loss: 2847185.2500 - val_mae: 1312.1921\n",
            "Epoch 423/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5178361.9688 - mae: 1328.2655 - val_loss: 3723178.5000 - val_mae: 1513.9186\n",
            "Epoch 424/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 10147305.2500 - mae: 1637.0324 - val_loss: 2855729.0000 - val_mae: 1294.5627\n",
            "Epoch 425/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 12993298.7500 - mae: 1737.9162 - val_loss: 2723357.5000 - val_mae: 1266.3627\n",
            "Epoch 426/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 4698075.3125 - mae: 1318.4275 - val_loss: 3060767.0000 - val_mae: 1329.7025\n",
            "Epoch 427/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7250538.9375 - mae: 1358.3688 - val_loss: 2748850.7500 - val_mae: 1244.6575\n",
            "Epoch 428/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 2985997.3750 - mae: 1136.7757 - val_loss: 3081092.5000 - val_mae: 1340.3146\n",
            "Epoch 429/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5907486.0000 - mae: 1417.4459 - val_loss: 5165613.0000 - val_mae: 1862.4382\n",
            "Epoch 430/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5824089.3125 - mae: 1341.7883 - val_loss: 2892072.7500 - val_mae: 1279.2775\n",
            "Epoch 431/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7104973.9375 - mae: 1310.4729 - val_loss: 3356945.0000 - val_mae: 1353.0043\n",
            "Epoch 432/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8528593.2917 - mae: 1308.0171 - val_loss: 2428500.7500 - val_mae: 1204.6743\n",
            "Epoch 433/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 5963618.8542 - mae: 1244.3805 - val_loss: 2339491.2500 - val_mae: 1182.9130\n",
            "Epoch 434/1000\n",
            "11/11 [==============================] - 0s 43ms/step - loss: 8823925.1875 - mae: 1497.8685 - val_loss: 3418843.7500 - val_mae: 1376.2603\n",
            "Epoch 435/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 4913220.6979 - mae: 1534.6856 - val_loss: 3985809.0000 - val_mae: 1607.1904\n",
            "Epoch 436/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 4437502.5417 - mae: 1555.8781 - val_loss: 4693536.0000 - val_mae: 1711.4519\n",
            "Epoch 437/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5138321.6875 - mae: 1692.2008 - val_loss: 8079289.0000 - val_mae: 2456.1658\n",
            "Epoch 438/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 7386543.4583 - mae: 1621.3073 - val_loss: 2626222.5000 - val_mae: 1230.0333\n",
            "Epoch 439/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6807406.9583 - mae: 1226.4553 - val_loss: 3725677.5000 - val_mae: 1567.7797\n",
            "Epoch 440/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3062856.9792 - mae: 1193.0051 - val_loss: 3893943.0000 - val_mae: 1584.9263\n",
            "Epoch 441/1000\n",
            "11/11 [==============================] - 0s 23ms/step - loss: 7687507.9375 - mae: 1452.6265 - val_loss: 2708314.7500 - val_mae: 1258.3958\n",
            "Epoch 442/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 3654834.1354 - mae: 1134.9681 - val_loss: 2537508.5000 - val_mae: 1191.3380\n",
            "Epoch 443/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 2516628.1979 - mae: 1103.9738 - val_loss: 4475384.0000 - val_mae: 1685.0090\n",
            "Epoch 444/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5375652.7292 - mae: 1427.3972 - val_loss: 2800242.0000 - val_mae: 1274.7137\n",
            "Epoch 445/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4831072.8958 - mae: 1240.2486 - val_loss: 4085251.2500 - val_mae: 1543.6494\n",
            "Epoch 446/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 8634447.6458 - mae: 1368.4068 - val_loss: 2707298.0000 - val_mae: 1236.1738\n",
            "Epoch 447/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3866400.1562 - mae: 1138.3828 - val_loss: 2285252.2500 - val_mae: 1133.4573\n",
            "Epoch 448/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 4454631.8229 - mae: 1132.5563 - val_loss: 3012085.0000 - val_mae: 1299.5260\n",
            "Epoch 449/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3810934.5417 - mae: 1343.7923 - val_loss: 3370268.0000 - val_mae: 1456.2188\n",
            "Epoch 450/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3684006.0000 - mae: 1300.6253 - val_loss: 5811553.5000 - val_mae: 2059.7351\n",
            "Epoch 451/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6350511.3750 - mae: 1505.9876 - val_loss: 2236865.0000 - val_mae: 1130.9830\n",
            "Epoch 452/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3051628.6146 - mae: 1096.1023 - val_loss: 4727850.0000 - val_mae: 1814.4082\n",
            "Epoch 453/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 10240489.6875 - mae: 1801.1372 - val_loss: 5224304.5000 - val_mae: 1801.5953\n",
            "Epoch 454/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3696415.3542 - mae: 1414.7053 - val_loss: 2665812.5000 - val_mae: 1237.2455\n",
            "Epoch 455/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3622714.0729 - mae: 1276.4163 - val_loss: 3241298.2500 - val_mae: 1384.9382\n",
            "Epoch 456/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 2855381.4792 - mae: 1098.1836 - val_loss: 2929766.7500 - val_mae: 1305.8030\n",
            "Epoch 457/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6811838.7083 - mae: 1511.5815 - val_loss: 2734555.7500 - val_mae: 1284.1837\n",
            "Epoch 458/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6666594.1771 - mae: 1248.6508 - val_loss: 2695710.7500 - val_mae: 1262.1459\n",
            "Epoch 459/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4982628.5312 - mae: 1239.6099 - val_loss: 3853247.7500 - val_mae: 1586.7518\n",
            "Epoch 460/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5731597.0625 - mae: 1329.2033 - val_loss: 2410801.7500 - val_mae: 1193.1390\n",
            "Epoch 461/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4014070.9167 - mae: 1425.3242 - val_loss: 2963654.2500 - val_mae: 1272.8627\n",
            "Epoch 462/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 2845500.7500 - mae: 1205.8850 - val_loss: 3216357.2500 - val_mae: 1398.6797\n",
            "Epoch 463/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3574300.0833 - mae: 1343.6307 - val_loss: 4093860.5000 - val_mae: 1628.0984\n",
            "Epoch 464/1000\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 5451979.1250 - mae: 1446.2950 - val_loss: 4206366.5000 - val_mae: 1551.5159\n",
            "Epoch 465/1000\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 4099254.9792 - mae: 1214.7809 - val_loss: 2553956.2500 - val_mae: 1217.1113\n",
            "Epoch 466/1000\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 4870271.1458 - mae: 1156.7891 - val_loss: 2791986.2500 - val_mae: 1260.7260\n",
            "Epoch 467/1000\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 4399521.8333 - mae: 1146.9993 - val_loss: 2363095.2500 - val_mae: 1156.9220\n",
            "Epoch 468/1000\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 7953752.0000 - mae: 1293.1340 - val_loss: 2754855.5000 - val_mae: 1261.4583\n",
            "Epoch 469/1000\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 7793092.4792 - mae: 1186.5497 - val_loss: 2438799.2500 - val_mae: 1186.5247\n",
            "Epoch 470/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 3618813.1458 - mae: 1048.2775 - val_loss: 2940150.7500 - val_mae: 1315.6763\n",
            "Epoch 471/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4632567.2604 - mae: 1263.5685 - val_loss: 4159441.5000 - val_mae: 1687.9661\n",
            "Epoch 472/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8086267.8750 - mae: 1654.4746 - val_loss: 4887685.5000 - val_mae: 1768.2977\n",
            "Epoch 473/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 11992367.0000 - mae: 1726.9413 - val_loss: 3253606.5000 - val_mae: 1379.1436\n",
            "Epoch 474/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5622179.9792 - mae: 1181.3072 - val_loss: 3136286.2500 - val_mae: 1401.9382\n",
            "Epoch 475/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5612335.0417 - mae: 1279.9608 - val_loss: 3374186.5000 - val_mae: 1450.1624\n",
            "Epoch 476/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 7246291.8542 - mae: 1457.3292 - val_loss: 3785950.7500 - val_mae: 1546.0381\n",
            "Epoch 477/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7969166.6875 - mae: 1273.0521 - val_loss: 2264612.0000 - val_mae: 1143.8013\n",
            "Epoch 478/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 1985417.1615 - mae: 956.7631 - val_loss: 2566515.7500 - val_mae: 1203.5439\n",
            "Epoch 479/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6429027.8542 - mae: 1207.3638 - val_loss: 3333056.7500 - val_mae: 1430.6915\n",
            "Epoch 480/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5996826.6667 - mae: 1309.8696 - val_loss: 2943314.2500 - val_mae: 1336.1807\n",
            "Epoch 481/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 9311633.2188 - mae: 1572.3615 - val_loss: 2993244.5000 - val_mae: 1316.8878\n",
            "Epoch 482/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5135979.0000 - mae: 1340.4639 - val_loss: 2298325.7500 - val_mae: 1152.2449\n",
            "Epoch 483/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 3909186.2812 - mae: 1034.1498 - val_loss: 2712642.2500 - val_mae: 1276.7322\n",
            "Epoch 484/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 2891650.9792 - mae: 1071.5595 - val_loss: 2955311.7500 - val_mae: 1361.8342\n",
            "Epoch 485/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4271626.5938 - mae: 1146.6150 - val_loss: 2356871.7500 - val_mae: 1166.7181\n",
            "Epoch 486/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6745323.7812 - mae: 1272.6837 - val_loss: 2676189.5000 - val_mae: 1240.7446\n",
            "Epoch 487/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 2810579.1354 - mae: 1215.5202 - val_loss: 2723564.0000 - val_mae: 1218.3960\n",
            "Epoch 488/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 8575471.1042 - mae: 1372.4630 - val_loss: 3509757.7500 - val_mae: 1512.9141\n",
            "Epoch 489/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6309045.4167 - mae: 1366.5425 - val_loss: 3221031.2500 - val_mae: 1412.1342\n",
            "Epoch 490/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 11916976.1250 - mae: 1570.2034 - val_loss: 3649075.0000 - val_mae: 1538.2922\n",
            "Epoch 491/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 9298712.8750 - mae: 1921.2985 - val_loss: 6267893.5000 - val_mae: 2072.4343\n",
            "Epoch 492/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8782130.0000 - mae: 1529.3107 - val_loss: 3403388.7500 - val_mae: 1408.7567\n",
            "Epoch 493/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3995096.3333 - mae: 1185.0511 - val_loss: 4865249.5000 - val_mae: 1765.9175\n",
            "Epoch 494/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5341233.6667 - mae: 1457.3429 - val_loss: 2772223.7500 - val_mae: 1288.3832\n",
            "Epoch 495/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 7920210.8125 - mae: 1269.8924 - val_loss: 3313150.5000 - val_mae: 1414.4955\n",
            "Epoch 496/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4371670.0208 - mae: 1199.4893 - val_loss: 2922650.7500 - val_mae: 1271.9100\n",
            "Epoch 497/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7165817.2917 - mae: 1388.0448 - val_loss: 3138871.0000 - val_mae: 1364.9854\n",
            "Epoch 498/1000\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 11486181.1250 - mae: 1617.1092 - val_loss: 2661062.2500 - val_mae: 1248.0984\n",
            "Epoch 499/1000\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 6259288.0833 - mae: 1164.6553 - val_loss: 3881227.0000 - val_mae: 1533.7971\n",
            "Epoch 500/1000\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 7059019.1042 - mae: 1545.4427 - val_loss: 3379956.5000 - val_mae: 1433.7765\n",
            "Epoch 501/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6603659.1354 - mae: 1256.4302 - val_loss: 3831949.0000 - val_mae: 1518.9628\n",
            "Epoch 502/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 7544971.7292 - mae: 1490.2429 - val_loss: 2540205.7500 - val_mae: 1224.3502\n",
            "Epoch 503/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5236281.0729 - mae: 1089.4036 - val_loss: 3414657.0000 - val_mae: 1437.5095\n",
            "Epoch 504/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5037805.2083 - mae: 1044.8056 - val_loss: 2901614.5000 - val_mae: 1301.5314\n",
            "Epoch 505/1000\n",
            "11/11 [==============================] - 0s 23ms/step - loss: 7830043.0625 - mae: 1243.8432 - val_loss: 2531437.5000 - val_mae: 1205.6639\n",
            "Epoch 506/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 6004530.4896 - mae: 1279.0320 - val_loss: 5004498.0000 - val_mae: 1876.6294\n",
            "Epoch 507/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 12783186.2917 - mae: 1837.7002 - val_loss: 4871348.0000 - val_mae: 1778.6654\n",
            "Epoch 508/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7105628.2917 - mae: 1793.0340 - val_loss: 5460652.5000 - val_mae: 1925.8204\n",
            "Epoch 509/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6936066.9792 - mae: 1411.5746 - val_loss: 4147062.5000 - val_mae: 1598.1620\n",
            "Epoch 510/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4040010.7917 - mae: 1369.7342 - val_loss: 6165455.0000 - val_mae: 2136.3853\n",
            "Epoch 511/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4107915.6667 - mae: 1416.8976 - val_loss: 2656182.5000 - val_mae: 1231.9507\n",
            "Epoch 512/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 2224577.6771 - mae: 1018.8644 - val_loss: 4073156.7500 - val_mae: 1624.8995\n",
            "Epoch 513/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5498465.5625 - mae: 1449.8690 - val_loss: 3517925.0000 - val_mae: 1467.6521\n",
            "Epoch 514/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 2536303.3438 - mae: 1111.3063 - val_loss: 4318471.0000 - val_mae: 1669.3384\n",
            "Epoch 515/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4837342.0833 - mae: 1452.2357 - val_loss: 3476203.0000 - val_mae: 1461.3184\n",
            "Epoch 516/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 8494834.3542 - mae: 1466.9692 - val_loss: 2551504.0000 - val_mae: 1174.6306\n",
            "Epoch 517/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5393304.2604 - mae: 1163.3096 - val_loss: 4079998.5000 - val_mae: 1606.0568\n",
            "Epoch 518/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 4719580.4583 - mae: 1279.5387 - val_loss: 4483591.0000 - val_mae: 1671.5970\n",
            "Epoch 519/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4203647.0000 - mae: 1355.8941 - val_loss: 3826143.5000 - val_mae: 1616.9666\n",
            "Epoch 520/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 2897119.5000 - mae: 1228.4927 - val_loss: 5385882.0000 - val_mae: 1914.0588\n",
            "Epoch 521/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 8558370.0000 - mae: 1486.8511 - val_loss: 3201660.7500 - val_mae: 1350.4934\n",
            "Epoch 522/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4630388.2083 - mae: 1211.7342 - val_loss: 3179690.2500 - val_mae: 1336.9800\n",
            "Epoch 523/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 7647382.8125 - mae: 1459.4743 - val_loss: 3163112.0000 - val_mae: 1383.1838\n",
            "Epoch 524/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3508629.5833 - mae: 1216.8807 - val_loss: 2988939.7500 - val_mae: 1344.4529\n",
            "Epoch 525/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3203641.1250 - mae: 1213.2173 - val_loss: 4139516.0000 - val_mae: 1623.2421\n",
            "Epoch 526/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 10620737.0000 - mae: 1491.9886 - val_loss: 3458369.0000 - val_mae: 1478.7830\n",
            "Epoch 527/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4305311.7083 - mae: 1153.6211 - val_loss: 2425569.0000 - val_mae: 1155.1509\n",
            "Epoch 528/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4599939.9688 - mae: 1142.0381 - val_loss: 3696556.5000 - val_mae: 1524.5177\n",
            "Epoch 529/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3558920.9167 - mae: 1200.1243 - val_loss: 2463414.0000 - val_mae: 1163.6732\n",
            "Epoch 530/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 3685765.0521 - mae: 1114.1655 - val_loss: 2658575.5000 - val_mae: 1199.1122\n",
            "Epoch 531/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3522048.8021 - mae: 1122.8275 - val_loss: 4843901.5000 - val_mae: 1824.2209\n",
            "Epoch 532/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4732566.2083 - mae: 1312.7245 - val_loss: 4183174.0000 - val_mae: 1593.4067\n",
            "Epoch 533/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 8421258.6875 - mae: 1727.4185 - val_loss: 3735438.7500 - val_mae: 1530.9548\n",
            "Epoch 534/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5528119.8958 - mae: 1532.2543 - val_loss: 3719406.2500 - val_mae: 1422.6185\n",
            "Epoch 535/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 5010709.6042 - mae: 1269.5259 - val_loss: 5956495.5000 - val_mae: 2065.2756\n",
            "Epoch 536/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4379610.7083 - mae: 1510.9690 - val_loss: 2434036.7500 - val_mae: 1179.1683\n",
            "Epoch 537/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 2657844.2396 - mae: 1089.5822 - val_loss: 3322453.0000 - val_mae: 1431.9768\n",
            "Epoch 538/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8540603.2500 - mae: 1436.7855 - val_loss: 2486694.0000 - val_mae: 1185.5991\n",
            "Epoch 539/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3671296.1250 - mae: 1228.0189 - val_loss: 2979308.2500 - val_mae: 1261.7621\n",
            "Epoch 540/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 2432070.8542 - mae: 1092.3131 - val_loss: 4165207.0000 - val_mae: 1607.7953\n",
            "Epoch 541/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 3967504.1458 - mae: 1505.4349 - val_loss: 3464923.0000 - val_mae: 1466.3749\n",
            "Epoch 542/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 4906418.1458 - mae: 1553.7337 - val_loss: 3580430.0000 - val_mae: 1500.4672\n",
            "Epoch 543/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4911750.1875 - mae: 1701.8885 - val_loss: 7107036.5000 - val_mae: 2169.9031\n",
            "Epoch 544/1000\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 11381496.7083 - mae: 2478.0142 - val_loss: 15747565.0000 - val_mae: 3659.7185\n",
            "Epoch 545/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 8063097.0833 - mae: 2334.3036 - val_loss: 4706487.5000 - val_mae: 1680.4843\n",
            "Epoch 546/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 6496951.1667 - mae: 1417.4482 - val_loss: 2945022.2500 - val_mae: 1323.6160\n",
            "Epoch 547/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 5135941.1458 - mae: 1201.2362 - val_loss: 2865077.7500 - val_mae: 1225.7305\n",
            "Epoch 548/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4097898.6875 - mae: 1233.8959 - val_loss: 4103688.2500 - val_mae: 1628.0038\n",
            "Epoch 549/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 4843859.0208 - mae: 1204.7902 - val_loss: 2773439.2500 - val_mae: 1239.3037\n",
            "Epoch 550/1000\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 10050301.9167 - mae: 1317.0113 - val_loss: 2489019.0000 - val_mae: 1215.1584\n",
            "Epoch 551/1000\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 2594687.6875 - mae: 964.4557 - val_loss: 3308749.2500 - val_mae: 1428.1466\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2541032.0000 - mae: 1262.4663\n",
            "loss, mae :  2541032.0 1262.46630859375\n",
            "RMSE :  1594.061363025406\n",
            "R2 :  0.9729504677582311\n",
            "[[91914.98]\n",
            " [88692.26]]\n",
            "월요일 삼성 시가 :  91915 원\n",
            "화요일 삼성 시가 :  88692 원\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGblTFOzk91e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40949532-7002-4640-8de8-4bc41e16cd13"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# 1. 데이터\r\n",
        "data1 = np.load('/content/drive/My Drive/data/test/samsung_jusik.npy')\r\n",
        "data2 = np.load('/content/drive/My Drive/data/test/kodex_jusik.npy')\r\n",
        "\r\n",
        "x1 = data1[1314:,:-1]\r\n",
        "x2 = data2\r\n",
        "y = data1[1314:,-1]\r\n",
        "print(x1.shape, x2.shape)\r\n",
        "print(y.shape)\r\n",
        "\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "scaler1 = MinMaxScaler()\r\n",
        "scaler1.fit(x1)\r\n",
        "x1 = scaler1.transform(x1)\r\n",
        "\r\n",
        "scaler2 = MinMaxScaler()\r\n",
        "scaler2.fit(x2)\r\n",
        "x2 = scaler2.transform(x2)\r\n",
        "\r\n",
        "size = 6\r\n",
        "\r\n",
        "def split_x(seq, size):\r\n",
        "    aaa = []\r\n",
        "    for i in range(len(seq)-size+1):\r\n",
        "        subset = seq[i : (i+size)]\r\n",
        "        aaa.append([item for item in subset])\r\n",
        "    print(type(aaa))\r\n",
        "    return np.array(aaa)\r\n",
        "x1_data = split_x(x1, size)\r\n",
        "x2_data = split_x(x2, size)\r\n",
        "\r\n",
        "x1 = x1_data[:-2,:,:]\r\n",
        "x2 = x2_data[:-2,:,:]\r\n",
        "\r\n",
        "\r\n",
        "y = y[7:]\r\n",
        "print(x1.shape, x2.shape)\r\n",
        "print(y.shape)\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "x1_train, x1_test, x2_train, x2_test, y_train, y_test = train_test_split(x1, x2, y, train_size=0.8, shuffle=True, random_state=66)\r\n",
        "\r\n",
        "\r\n",
        "#2. 모델구성\r\n",
        "from tensorflow.keras.models import Model, load_model\r\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Dropout, Conv1D, Flatten, MaxPooling1D, GRU, SimpleRNN\r\n",
        "\r\n",
        "# # 모델 1\r\n",
        "# input1 = Input(shape=(6,x1.shape[2]))\r\n",
        "# dense1 = LSTM(512)(input1)\r\n",
        "# dense1 = Dense(256)(dense1)\r\n",
        "# dense1 = Dense(128)(dense1)\r\n",
        "# dense1 = Dense(64)(dense1)\r\n",
        "# dense1 = Dense(32)(dense1)\r\n",
        "# dense1 = Dense(16)(dense1)\r\n",
        "# dense1 = Dense(8)(dense1)\r\n",
        "# dense1 = Dense(4)(dense1)\r\n",
        "# dense1 = Dense(2)(dense1)\r\n",
        "\r\n",
        "\r\n",
        "# # 모델 2\r\n",
        "# input2 = Input(shape=(6,x2.shape[2]))\r\n",
        "# dense2 = LSTM(512)(input2)\r\n",
        "# dense2 = Dense(256)(dense2)\r\n",
        "# dense2 = Dense(128)(dense2)\r\n",
        "# dense2 = Dense(64)(dense2)\r\n",
        "# dense2 = Dense(32)(dense2)\r\n",
        "# dense2 = Dense(16)(dense2)\r\n",
        "# dense2 = Dense(8)(dense2)\r\n",
        "# dense2 = Dense(4)(dense2)\r\n",
        "# dense2 = Dense(2)(dense2)\r\n",
        "\r\n",
        "\r\n",
        "# from tensorflow.keras.layers import concatenate, Concatenate\r\n",
        "# merge1 = concatenate([dense1, dense2])\r\n",
        "# middle1 = Dense(128)(merge1)\r\n",
        "# middle1 = Dense(64)(middle1)\r\n",
        "# middle1 = Dense(32)(middle1)\r\n",
        "# middle1 = Dense(16)(middle1)\r\n",
        "# middle1 = Dense(8)(middle1)\r\n",
        "# middle1 = Dense(4)(middle1)\r\n",
        "# middle1 = Dense(2)(middle1)\r\n",
        "# outputs = Dense(1)(middle1)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# model = Model(inputs=[input1,input2], outputs=outputs)\r\n",
        "\r\n",
        "\r\n",
        "# #3. 컴파일, 훈련\r\n",
        "# model.compile(loss='mse', optimizer='adam', metrics=['mae'])\r\n",
        "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "# es = EarlyStopping(monitor='val_loss', patience=100, mode='auto')\r\n",
        "# modelpath= '/content/drive/My Drive/data/test/modelcheckpoint/samsung_checkpoint_{val_loss:.4f}.hdf5'\r\n",
        "# cp = ModelCheckpoint(modelpath, monitor='val_loss', save_best_only=True, mode='auto')\r\n",
        "# model.fit([x1_train,x2_train], y_train, batch_size=64, epochs=1000, validation_split=0.2, callbacks=[cp, es])\r\n",
        "\r\n",
        "# model = load_model('/content/drive/My Drive/data/test/samsung_model2.h5')\r\n",
        "model = load_model('/content/drive/My Drive/data/test/modelcheckpoint/samsung_checkpoint_2285252.2500.hdf5')\r\n",
        "\r\n",
        "\r\n",
        "#4. 평가, 예측\r\n",
        "loss, mae = model.evaluate([x1_test,x2_test], y_test, batch_size=64)\r\n",
        "y_predict = model.predict([x1_test, x2_test])\r\n",
        "print(\"loss, mae : \", loss, mae)\r\n",
        "\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "def RMSE(y_test, y_predict):\r\n",
        "    return np.sqrt(mean_squared_error(y_test, y_predict))\r\n",
        "print(\"RMSE : \", RMSE(y_test, y_predict))\r\n",
        "\r\n",
        "from sklearn.metrics import r2_score\r\n",
        "r2 = r2_score(y_test, y_predict)\r\n",
        "print(\"R2 : \", r2)\r\n",
        "\r\n",
        "x1_pred = x1_data[-2:,:,:]\r\n",
        "x2_pred = x2_data[-2:,:,:]\r\n",
        "x1_pred = x1_pred.reshape(x1_pred.shape[0],6,x1_pred.shape[-1])\r\n",
        "x2_pred = x2_pred.reshape(x2_pred.shape[0],6,x2_pred.shape[-1])\r\n",
        "y_predict = model.predict([x1_pred,x2_pred])\r\n",
        "\r\n",
        "print(y_predict)\r\n",
        "\r\n",
        "\r\n",
        "print(\"월요일 삼성 시가 : \", int(np.round(y_predict[0])), \"원\")\r\n",
        "print(\"화요일 삼성 시가 : \", int(np.round(y_predict[1])), \"원\")\r\n",
        "\r\n",
        "# 로드 모델\r\n",
        "#loss, mae :  2541032.0 1262.46630859375\r\n",
        "# RMSE :  1594.061363025406\r\n",
        "# R2 :  0.9729504677582311\r\n",
        "# [[91914.98]\r\n",
        "#  [88692.26]]\r\n",
        "# 월요일 삼성 시가 :  91915 원\r\n",
        "# 화요일 삼성 시가 :  88692 원\r\n",
        "\r\n",
        "# check\r\n",
        "# loss, mae :  2288014.0 1166.26708984375\r\n",
        "# RMSE :  1512.6178212770928\r\n",
        "# R2 :  0.9756438794913104\r\n",
        "# [[90409.25]\r\n",
        "#  [88900.08]]\r\n",
        "# 월요일 삼성 시가 :  90409 원\r\n",
        "# 화요일 삼성 시가 :  88900 원\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1085, 14) (1085, 14)\n",
            "(1085,)\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "(1078, 6, 14) (1078, 6, 14)\n",
            "(1078,)\n",
            "4/4 [==============================] - 1s 6ms/step - loss: 2288014.0000 - mae: 1166.2671\n",
            "loss, mae :  2288014.0 1166.26708984375\n",
            "RMSE :  1512.6178212770928\n",
            "R2 :  0.9756438794913104\n",
            "[[90409.25]\n",
            " [88900.08]]\n",
            "월요일 삼성 시가 :  90409 원\n",
            "화요일 삼성 시가 :  88900 원\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}