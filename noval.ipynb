{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab 구독 최대한 활용하기",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunghoonSeok/Study/blob/master/noval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Colab 구독 최대한 활용하기\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr3WPRYDBWmK",
        "outputId": "dd9e8612-81cc-42f6-86ab-cb61c5aa736d"
      },
      "source": [
        "import os\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJavX6ngB8-5"
      },
      "source": [
        "\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "for i in range(50000):\r\n",
        "    image_path = '/content/drive/My Drive/dirty_mnist_/dirty_mnist_2nd/%05d.png'%i\r\n",
        "    image = cv2.imread(image_path)\r\n",
        "    image2 = np.where((image <= 254) & (image != 0), 0, image)\r\n",
        "    image3 = cv2.dilate(image2, kernel=np.ones((2, 2), np.uint8), iterations=1)\r\n",
        "    image4 = cv2.medianBlur(src=image3, ksize= 5)\r\n",
        "    cv2.imwrite('/content/drive/My Drive/dirty_mnist_/dirty_mnist_clean/%05d.png'%i, image4)\r\n",
        "\r\n",
        "for i in range(50000,55000):\r\n",
        "    image_path = '/content/drive/My Drive/dirty_mnist_/test_dirty_mnist_2nd/%05d.png'%i\r\n",
        "    image = cv2.imread(image_path)\r\n",
        "    image2 = np.where((image <= 254) & (image != 0), 0, image)\r\n",
        "    image3 = cv2.dilate(image2, kernel=np.ones((2, 2), np.uint8), iterations=1)\r\n",
        "    image4 = cv2.medianBlur(src=image3, ksize= 5)\r\n",
        "    cv2.imwrite('/content/drive/My Drive/dirty_mnist_/test_dirty_mnist_clean/%05d.png'%i, image4)\r\n",
        "\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvT84qUAEwTW",
        "outputId": "611d009f-b0d5-4439-e72e-fbc361bdb4d9"
      },
      "source": [
        "import os\r\n",
        "from typing import Tuple, Sequence, Callable\r\n",
        "import csv\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from PIL import Image\r\n",
        "import torch\r\n",
        "import torch.optim as optim\r\n",
        "from torch import nn, Tensor\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "# from torchinfo import summary\r\n",
        "\r\n",
        "from torchvision import transforms, utils\r\n",
        "from torchvision.models import resnet50\r\n",
        "from skimage import io, transform\r\n",
        "\r\n",
        "class MnistDataset(Dataset):\r\n",
        "    def __init__(self,dir: os.PathLike,image_ids: os.PathLike,transforms: Sequence[Callable]) -> None:\r\n",
        "        self.dir = dir\r\n",
        "        self.transforms = transforms\r\n",
        "\r\n",
        "        self.labels = {}\r\n",
        "        with open(image_ids, 'r') as f:\r\n",
        "            reader = csv.reader(f)\r\n",
        "            next(reader)\r\n",
        "            for row in reader:\r\n",
        "                self.labels[int(row[0])] = list(map(int, row[1:]))\r\n",
        "\r\n",
        "        self.image_ids = list(self.labels.keys())\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self.image_ids)\r\n",
        "\r\n",
        "    def __getitem__(self, index: int) -> Tuple[Tensor]:\r\n",
        "        image_id = self.image_ids[index]\r\n",
        "        image = Image.open(\r\n",
        "            os.path.join(\r\n",
        "                self.dir, f'{str(image_id).zfill(5)}.png')).resize((128,128)).convert('RGB')\r\n",
        "        target = np.array(self.labels.get(image_id)).astype(np.float32)\r\n",
        "\r\n",
        "        if self.transforms is not None:\r\n",
        "            image = self.transforms(image)\r\n",
        "\r\n",
        "        return image, target\r\n",
        "\r\n",
        "transforms_train = transforms.Compose([\r\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\r\n",
        "    transforms.RandomVerticalFlip(p=0.5),\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize(\r\n",
        "        [0.485, 0.456, 0.406],\r\n",
        "        [0.229, 0.224, 0.225]\r\n",
        "    )\r\n",
        "])\r\n",
        "\r\n",
        "transforms_test = transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize(\r\n",
        "        [0.485, 0.456, 0.406],\r\n",
        "        [0.229, 0.224, 0.225]\r\n",
        "    )\r\n",
        "])\r\n",
        "\r\n",
        "trainset = MnistDataset('/content/drive/My Drive/dirty_mnist_/dirty_mnist_clean/', '/content/drive/My Drive/dirty_mnist_/dirty_mnist_2nd_answer.csv', transforms_train)\r\n",
        "testset = MnistDataset('/content/drive/My Drive/dirty_mnist_/test_dirty_mnist_clean/', '/content/drive/My Drive/dirty_mnist_/sample_submission.csv', transforms_test)\r\n",
        "\r\n",
        "train_loader = DataLoader(trainset, batch_size=128, num_workers=8)\r\n",
        "test_loader = DataLoader(testset, batch_size=32, num_workers=4)\r\n",
        "\r\n",
        "class MnistModel(nn.Module):\r\n",
        "    def __init__(self) -> None:\r\n",
        "        super().__init__()\r\n",
        "        self.resnet = resnet50(pretrained=True)\r\n",
        "        self.classifier = nn.Linear(1000, 26)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.resnet(x)\r\n",
        "        x = self.classifier(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "model = MnistModel().to(device)\r\n",
        "# print(summary(model, input_size=(1, 3, 128, 128), verbose=1))\r\n",
        "\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\r\n",
        "criterion = nn.MultiLabelSoftMarginLoss()\r\n",
        "\r\n",
        "num_epochs = 40\r\n",
        "model.train()\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    for i, (images, targets) in enumerate(train_loader):\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        images = images.to(device)\r\n",
        "        targets = targets.to(device)\r\n",
        "\r\n",
        "        outputs = model(images)\r\n",
        "        loss = criterion(outputs, targets)\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        if (i+1) % 10 == 0:\r\n",
        "            outputs = outputs > 0.5\r\n",
        "            acc = (outputs == targets).float().mean()\r\n",
        "            torch.save(model.state_dict(), os.path.join('/content/drive/My Drive/dirty_mnist_/checkpoint/', f'pretrained_model{epoch}.pth'))\r\n",
        "            print(f'{epoch}: {loss.item():.5f}, {acc.item():.5f}')\r\n",
        "\r\n",
        "submit = pd.read_csv('/content/drive/My Drive/dirty_mnist_/sample_submission.csv')\r\n",
        "\r\n",
        "model.eval()\r\n",
        "batch_size = test_loader.batch_size\r\n",
        "batch_index = 0\r\n",
        "for i, (images, targets) in enumerate(test_loader):\r\n",
        "    images = images.to(device)\r\n",
        "    targets = targets.to(device)\r\n",
        "    outputs = model(images)\r\n",
        "    outputs = outputs > 0.5\r\n",
        "    batch_index = i * batch_size\r\n",
        "    submit.iloc[batch_index:batch_index+batch_size, 1:] = \\\r\n",
        "        outputs.long().squeeze(0).detach().cpu().numpy()\r\n",
        "    \r\n",
        "submit.to_csv('/content/drive/My Drive/dirty_mnist_/submission_torch2.csv', index=False)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 0.71403, 0.54056\n",
            "0: 0.69937, 0.54056\n",
            "0: 0.69390, 0.53125\n",
            "0: 0.69379, 0.53486\n",
            "0: 0.68992, 0.54117\n",
            "0: 0.69495, 0.54778\n",
            "0: 0.68688, 0.53756\n",
            "0: 0.68675, 0.55439\n",
            "0: 0.69036, 0.54387\n",
            "0: 0.68282, 0.54207\n",
            "0: 0.69159, 0.54177\n",
            "0: 0.68101, 0.54117\n",
            "0: 0.67897, 0.55379\n",
            "0: 0.68765, 0.54177\n",
            "0: 0.68182, 0.54688\n",
            "0: 0.68139, 0.54958\n",
            "0: 0.67884, 0.54657\n",
            "0: 0.68079, 0.55379\n",
            "0: 0.68251, 0.54657\n",
            "0: 0.68540, 0.54237\n",
            "0: 0.67930, 0.55919\n",
            "0: 0.68167, 0.54928\n",
            "0: 0.67254, 0.56070\n",
            "0: 0.68224, 0.55589\n",
            "0: 0.68435, 0.55349\n",
            "0: 0.67895, 0.55679\n",
            "0: 0.67511, 0.55980\n",
            "0: 0.67283, 0.54718\n",
            "0: 0.67094, 0.55679\n",
            "0: 0.67316, 0.55499\n",
            "0: 0.67786, 0.55258\n",
            "0: 0.66940, 0.55529\n",
            "0: 0.66766, 0.57422\n",
            "0: 0.66432, 0.56340\n",
            "0: 0.66336, 0.56911\n",
            "0: 0.66800, 0.56611\n",
            "0: 0.66448, 0.56911\n",
            "0: 0.66642, 0.56941\n",
            "0: 0.66321, 0.56520\n",
            "1: 0.66650, 0.57091\n",
            "1: 0.66259, 0.56911\n",
            "1: 0.66486, 0.56581\n",
            "1: 0.66672, 0.55980\n",
            "1: 0.65718, 0.57061\n",
            "1: 0.66870, 0.56190\n",
            "1: 0.66314, 0.57242\n",
            "1: 0.66303, 0.58023\n",
            "1: 0.66955, 0.57242\n",
            "1: 0.66353, 0.57091\n",
            "1: 0.66108, 0.57452\n",
            "1: 0.66464, 0.57512\n",
            "1: 0.65889, 0.57482\n",
            "1: 0.66192, 0.57812\n",
            "1: 0.65956, 0.57933\n",
            "1: 0.66221, 0.58143\n",
            "1: 0.66131, 0.57302\n",
            "1: 0.66871, 0.57332\n",
            "1: 0.66185, 0.57812\n",
            "1: 0.65970, 0.57332\n",
            "1: 0.65197, 0.59615\n",
            "1: 0.64957, 0.58864\n",
            "1: 0.64500, 0.59345\n",
            "1: 0.65505, 0.58864\n",
            "1: 0.66152, 0.59075\n",
            "1: 0.65338, 0.58984\n",
            "1: 0.64368, 0.60126\n",
            "1: 0.65230, 0.58444\n",
            "1: 0.64151, 0.59525\n",
            "1: 0.64866, 0.59075\n",
            "1: 0.65145, 0.58504\n",
            "1: 0.64224, 0.59405\n",
            "1: 0.65301, 0.59375\n",
            "1: 0.64392, 0.59946\n",
            "1: 0.64559, 0.59826\n",
            "1: 0.64603, 0.59375\n",
            "1: 0.64202, 0.60337\n",
            "1: 0.64766, 0.59976\n",
            "1: 0.64435, 0.60397\n",
            "2: 0.63635, 0.60276\n",
            "2: 0.64207, 0.58924\n",
            "2: 0.63615, 0.59856\n",
            "2: 0.64444, 0.59856\n",
            "2: 0.62966, 0.60998\n",
            "2: 0.63892, 0.59886\n",
            "2: 0.64056, 0.60637\n",
            "2: 0.64132, 0.59946\n",
            "2: 0.63634, 0.60757\n",
            "2: 0.62969, 0.60727\n",
            "2: 0.63557, 0.61388\n",
            "2: 0.64160, 0.60787\n",
            "2: 0.63831, 0.60306\n",
            "2: 0.63681, 0.59555\n",
            "2: 0.62861, 0.61328\n",
            "2: 0.64152, 0.60667\n",
            "2: 0.63488, 0.60216\n",
            "2: 0.64654, 0.60367\n",
            "2: 0.63638, 0.61088\n",
            "2: 0.63839, 0.60337\n",
            "2: 0.63246, 0.62410\n",
            "2: 0.62516, 0.61779\n",
            "2: 0.62851, 0.60787\n",
            "2: 0.63504, 0.62169\n",
            "2: 0.63196, 0.61599\n",
            "2: 0.63551, 0.61268\n",
            "2: 0.63156, 0.61959\n",
            "2: 0.63752, 0.61088\n",
            "2: 0.62405, 0.61689\n",
            "2: 0.62474, 0.61869\n",
            "2: 0.63311, 0.61388\n",
            "2: 0.62072, 0.62200\n",
            "2: 0.63152, 0.61298\n",
            "2: 0.62810, 0.61899\n",
            "2: 0.62326, 0.62019\n",
            "2: 0.63126, 0.61358\n",
            "2: 0.62115, 0.62680\n",
            "2: 0.62869, 0.61959\n",
            "2: 0.61840, 0.62200\n",
            "3: 0.62198, 0.61959\n",
            "3: 0.62656, 0.61569\n",
            "3: 0.62475, 0.61689\n",
            "3: 0.62385, 0.60877\n",
            "3: 0.61042, 0.62951\n",
            "3: 0.63290, 0.60757\n",
            "3: 0.62703, 0.62560\n",
            "3: 0.62041, 0.62710\n",
            "3: 0.63055, 0.61839\n",
            "3: 0.61957, 0.61719\n",
            "3: 0.62448, 0.62109\n",
            "3: 0.62502, 0.62169\n",
            "3: 0.62828, 0.61869\n",
            "3: 0.61624, 0.62831\n",
            "3: 0.61783, 0.62650\n",
            "3: 0.62049, 0.62290\n",
            "3: 0.62685, 0.61298\n",
            "3: 0.62299, 0.62560\n",
            "3: 0.61172, 0.62680\n",
            "3: 0.62455, 0.62139\n",
            "3: 0.61406, 0.65174\n",
            "3: 0.60822, 0.63371\n",
            "3: 0.61713, 0.62710\n",
            "3: 0.62364, 0.64483\n",
            "3: 0.61875, 0.63401\n",
            "3: 0.61643, 0.63281\n",
            "3: 0.61307, 0.64393\n",
            "3: 0.62111, 0.63131\n",
            "3: 0.60768, 0.64032\n",
            "3: 0.61293, 0.63612\n",
            "3: 0.61260, 0.62470\n",
            "3: 0.60238, 0.64904\n",
            "3: 0.62186, 0.62139\n",
            "3: 0.61118, 0.63912\n",
            "3: 0.68340, 0.57632\n",
            "3: 0.67442, 0.55799\n",
            "3: 0.65845, 0.57272\n",
            "3: 0.66115, 0.57181\n",
            "3: 0.65135, 0.58864\n",
            "4: 0.64665, 0.58594\n",
            "4: 0.64311, 0.58714\n",
            "4: 0.63833, 0.59405\n",
            "4: 0.63631, 0.60036\n",
            "4: 0.62911, 0.61178\n",
            "4: 0.63177, 0.60877\n",
            "4: 0.63375, 0.60547\n",
            "4: 0.62528, 0.62530\n",
            "4: 0.63831, 0.61178\n",
            "4: 0.62885, 0.61388\n",
            "4: 0.62393, 0.61749\n",
            "4: 0.63074, 0.61899\n",
            "4: 0.63013, 0.61388\n",
            "4: 0.62600, 0.61779\n",
            "4: 0.61120, 0.62921\n",
            "4: 0.62353, 0.62831\n",
            "4: 0.61668, 0.63732\n",
            "4: 0.62474, 0.63281\n",
            "4: 0.61410, 0.63251\n",
            "4: 0.62015, 0.62350\n",
            "4: 0.61216, 0.64663\n",
            "4: 0.60622, 0.64093\n",
            "4: 0.61416, 0.63311\n",
            "4: 0.61557, 0.63732\n",
            "4: 0.62238, 0.63011\n",
            "4: 0.61596, 0.63732\n",
            "4: 0.61013, 0.64333\n",
            "4: 0.61511, 0.63702\n",
            "4: 0.59369, 0.64964\n",
            "4: 0.60318, 0.64423\n",
            "4: 0.61429, 0.62921\n",
            "4: 0.59206, 0.65024\n",
            "4: 0.61282, 0.64093\n",
            "4: 0.60923, 0.64032\n",
            "4: 0.60708, 0.64243\n",
            "4: 0.61088, 0.64032\n",
            "4: 0.58986, 0.66286\n",
            "4: 0.61242, 0.63612\n",
            "4: 0.60881, 0.64393\n",
            "5: 0.59792, 0.64393\n",
            "5: 0.61900, 0.62740\n",
            "5: 0.60187, 0.63792\n",
            "5: 0.61073, 0.63702\n",
            "5: 0.58565, 0.65475\n",
            "5: 0.59975, 0.65174\n",
            "5: 0.60222, 0.64363\n",
            "5: 0.59878, 0.65325\n",
            "5: 0.60138, 0.65024\n",
            "5: 0.59292, 0.65234\n",
            "5: 0.59833, 0.65655\n",
            "5: 0.60155, 0.65024\n",
            "5: 0.60247, 0.65294\n",
            "5: 0.59995, 0.64603\n",
            "5: 0.58228, 0.66917\n",
            "5: 0.59388, 0.65986\n",
            "5: 0.60477, 0.64333\n",
            "5: 0.61352, 0.64363\n",
            "5: 0.58936, 0.65325\n",
            "5: 0.59907, 0.64663\n",
            "5: 0.59523, 0.66587\n",
            "5: 0.59490, 0.65445\n",
            "5: 0.59463, 0.65144\n",
            "5: 0.60167, 0.65294\n",
            "5: 0.60728, 0.64874\n",
            "5: 0.59792, 0.66106\n",
            "5: 0.58960, 0.66436\n",
            "5: 0.59779, 0.65565\n",
            "5: 0.57550, 0.67518\n",
            "5: 0.58537, 0.66316\n",
            "5: 0.59089, 0.65054\n",
            "5: 0.57593, 0.67067\n",
            "5: 0.59577, 0.65715\n",
            "5: 0.59173, 0.66346\n",
            "5: 0.58921, 0.65865\n",
            "5: 0.59404, 0.66346\n",
            "5: 0.57265, 0.67097\n",
            "5: 0.59539, 0.65865\n",
            "5: 0.58820, 0.66737\n",
            "6: 0.57596, 0.66526\n",
            "6: 0.59284, 0.65956\n",
            "6: 0.58759, 0.64724\n",
            "6: 0.59009, 0.66016\n",
            "6: 0.56188, 0.68089\n",
            "6: 0.58384, 0.67398\n",
            "6: 0.58470, 0.66526\n",
            "6: 0.57395, 0.68780\n",
            "6: 0.58962, 0.66376\n",
            "6: 0.57090, 0.67728\n",
            "6: 0.58354, 0.66917\n",
            "6: 0.58347, 0.66917\n",
            "6: 0.58788, 0.66046\n",
            "6: 0.59245, 0.65865\n",
            "6: 0.57091, 0.68209\n",
            "6: 0.59269, 0.65865\n",
            "6: 0.59314, 0.66016\n",
            "6: 0.59436, 0.65986\n",
            "6: 0.58119, 0.66556\n",
            "6: 0.58869, 0.65805\n",
            "6: 0.57604, 0.68389\n",
            "6: 0.57598, 0.67308\n",
            "6: 0.57549, 0.66587\n",
            "6: 0.59497, 0.66677\n",
            "6: 0.58538, 0.66947\n",
            "6: 0.57999, 0.67097\n",
            "6: 0.58069, 0.67638\n",
            "6: 0.59519, 0.65956\n",
            "6: 0.56720, 0.68600\n",
            "6: 0.58077, 0.67248\n",
            "6: 0.57576, 0.66647\n",
            "6: 0.55645, 0.69231\n",
            "6: 0.58407, 0.66376\n",
            "6: 0.57946, 0.68359\n",
            "6: 0.57669, 0.67638\n",
            "6: 0.57896, 0.68269\n",
            "6: 0.56415, 0.67909\n",
            "6: 0.58576, 0.67097\n",
            "6: 0.56995, 0.68209\n",
            "7: 0.57436, 0.67308\n",
            "7: 0.57363, 0.67638\n",
            "7: 0.57586, 0.67518\n",
            "7: 0.57692, 0.67518\n",
            "7: 0.55379, 0.68810\n",
            "7: 0.57775, 0.68119\n",
            "7: 0.55934, 0.68239\n",
            "7: 0.56119, 0.69081\n",
            "7: 0.57787, 0.67788\n",
            "7: 0.56017, 0.68419\n",
            "7: 0.56488, 0.67939\n",
            "7: 0.56790, 0.68059\n",
            "7: 0.57517, 0.68149\n",
            "7: 0.58206, 0.67248\n",
            "7: 0.55161, 0.69952\n",
            "7: 0.56499, 0.69020\n",
            "7: 0.57339, 0.68089\n",
            "7: 0.57613, 0.68239\n",
            "7: 0.55352, 0.69411\n",
            "7: 0.56669, 0.68570\n",
            "7: 0.56330, 0.69982\n",
            "7: 0.56012, 0.69501\n",
            "7: 0.56448, 0.68510\n",
            "7: 0.58309, 0.67548\n",
            "7: 0.56997, 0.68450\n",
            "7: 0.56376, 0.69712\n",
            "7: 0.56355, 0.68840\n",
            "7: 0.57918, 0.67007\n",
            "7: 0.54671, 0.69712\n",
            "7: 0.56040, 0.69621\n",
            "7: 0.56223, 0.68239\n",
            "7: 0.54205, 0.70733\n",
            "7: 0.57200, 0.67849\n",
            "7: 0.57254, 0.67788\n",
            "7: 0.55722, 0.69261\n",
            "7: 0.56309, 0.69621\n",
            "7: 0.55864, 0.69050\n",
            "7: 0.57380, 0.68179\n",
            "7: 0.56920, 0.68299\n",
            "8: 0.54702, 0.69441\n",
            "8: 0.55563, 0.68780\n",
            "8: 0.55507, 0.68780\n",
            "8: 0.56351, 0.68690\n",
            "8: 0.52731, 0.71124\n",
            "8: 0.55464, 0.69621\n",
            "8: 0.54219, 0.69892\n",
            "8: 0.54875, 0.70373\n",
            "8: 0.55792, 0.69531\n",
            "8: 0.54447, 0.70703\n",
            "8: 0.55895, 0.69441\n",
            "8: 0.55028, 0.69892\n",
            "8: 0.55854, 0.69772\n",
            "8: 0.57024, 0.69471\n",
            "8: 0.54720, 0.70823\n",
            "8: 0.55243, 0.69832\n",
            "8: 0.55516, 0.69742\n",
            "8: 0.56764, 0.69050\n",
            "8: 0.54158, 0.69651\n",
            "8: 0.55570, 0.70102\n",
            "8: 0.54738, 0.71454\n",
            "8: 0.55224, 0.69802\n",
            "8: 0.54963, 0.70072\n",
            "8: 0.56396, 0.70192\n",
            "8: 0.55125, 0.70583\n",
            "8: 0.55398, 0.70222\n",
            "8: 0.54815, 0.70463\n",
            "8: 0.57050, 0.68359\n",
            "8: 0.52633, 0.71815\n",
            "8: 0.54859, 0.70553\n",
            "8: 0.54933, 0.70343\n",
            "8: 0.52372, 0.71875\n",
            "8: 0.55115, 0.70102\n",
            "8: 0.55221, 0.69651\n",
            "8: 0.54845, 0.70192\n",
            "8: 0.54982, 0.70613\n",
            "8: 0.53761, 0.70913\n",
            "8: 0.55233, 0.70222\n",
            "8: 0.54602, 0.71094\n",
            "9: 0.53270, 0.71214\n",
            "9: 0.54663, 0.69862\n",
            "9: 0.54981, 0.70162\n",
            "9: 0.54216, 0.70403\n",
            "9: 0.52597, 0.72175\n",
            "9: 0.53467, 0.71154\n",
            "9: 0.53935, 0.70583\n",
            "9: 0.53411, 0.72025\n",
            "9: 0.55078, 0.70162\n",
            "9: 0.52338, 0.71965\n",
            "9: 0.53874, 0.70823\n",
            "9: 0.54115, 0.71094\n",
            "9: 0.53642, 0.70703\n",
            "9: 0.54872, 0.70553\n",
            "9: 0.51587, 0.73047\n",
            "9: 0.53268, 0.71785\n",
            "9: 0.54412, 0.70553\n",
            "9: 0.54527, 0.70913\n",
            "9: 0.53309, 0.71304\n",
            "9: 0.54550, 0.70403\n",
            "9: 0.53717, 0.71695\n",
            "9: 0.52732, 0.71665\n",
            "9: 0.53128, 0.71214\n",
            "9: 0.53943, 0.71725\n",
            "9: 0.53384, 0.71575\n",
            "9: 0.52271, 0.73167\n",
            "9: 0.54171, 0.71484\n",
            "9: 0.55128, 0.69531\n",
            "9: 0.51239, 0.72867\n",
            "9: 0.53354, 0.71575\n",
            "9: 0.53272, 0.71274\n",
            "9: 0.50918, 0.73708\n",
            "9: 0.53661, 0.71605\n",
            "9: 0.54118, 0.71244\n",
            "9: 0.54140, 0.71605\n",
            "9: 0.53585, 0.71094\n",
            "9: 0.53339, 0.71124\n",
            "9: 0.54689, 0.71064\n",
            "9: 0.53255, 0.72296\n",
            "10: 0.51981, 0.71755\n",
            "10: 0.53257, 0.71154\n",
            "10: 0.52569, 0.71785\n",
            "10: 0.53594, 0.71334\n",
            "10: 0.50661, 0.73347\n",
            "10: 0.53177, 0.72025\n",
            "10: 0.52389, 0.72145\n",
            "10: 0.52191, 0.72416\n",
            "10: 0.53291, 0.71875\n",
            "10: 0.51947, 0.72746\n",
            "10: 0.52294, 0.72386\n",
            "10: 0.52206, 0.72927\n",
            "10: 0.52572, 0.71725\n",
            "10: 0.54222, 0.71575\n",
            "10: 0.51474, 0.73678\n",
            "10: 0.52534, 0.72416\n",
            "10: 0.53759, 0.71304\n",
            "10: 0.53929, 0.71214\n",
            "10: 0.51826, 0.72897\n",
            "10: 0.52218, 0.72746\n",
            "10: 0.52414, 0.73107\n",
            "10: 0.52478, 0.71935\n",
            "10: 0.51437, 0.73257\n",
            "10: 0.52774, 0.72326\n",
            "10: 0.52598, 0.72266\n",
            "10: 0.51945, 0.72776\n",
            "10: 0.52498, 0.72536\n",
            "10: 0.55324, 0.71034\n",
            "10: 0.50742, 0.72656\n",
            "10: 0.52215, 0.72776\n",
            "10: 0.52586, 0.71725\n",
            "10: 0.49373, 0.74219\n",
            "10: 0.52325, 0.72776\n",
            "10: 0.52359, 0.72656\n",
            "10: 0.52048, 0.71785\n",
            "10: 0.52038, 0.71815\n",
            "10: 0.51834, 0.72266\n",
            "10: 0.53709, 0.71094\n",
            "10: 0.52195, 0.73077\n",
            "11: 0.51312, 0.73107\n",
            "11: 0.51539, 0.73167\n",
            "11: 0.51693, 0.72686\n",
            "11: 0.52534, 0.72115\n",
            "11: 0.49126, 0.74519\n",
            "11: 0.52019, 0.72776\n",
            "11: 0.51043, 0.73047\n",
            "11: 0.50555, 0.74639\n",
            "11: 0.51955, 0.72776\n",
            "11: 0.50699, 0.73828\n",
            "11: 0.50819, 0.73738\n",
            "11: 0.51317, 0.73618\n",
            "11: 0.51273, 0.72867\n",
            "11: 0.52678, 0.72957\n",
            "11: 0.50106, 0.74429\n",
            "11: 0.50578, 0.73347\n",
            "11: 0.51921, 0.72596\n",
            "11: 0.52292, 0.72626\n",
            "11: 0.50603, 0.74099\n",
            "11: 0.52093, 0.72115\n",
            "11: 0.49969, 0.74489\n",
            "11: 0.51629, 0.72897\n",
            "11: 0.50851, 0.73618\n",
            "11: 0.51642, 0.73317\n",
            "11: 0.50986, 0.73768\n",
            "11: 0.50531, 0.73798\n",
            "11: 0.51007, 0.73798\n",
            "11: 0.53309, 0.72145\n",
            "11: 0.48937, 0.74489\n",
            "11: 0.50633, 0.73498\n",
            "11: 0.51599, 0.72716\n",
            "11: 0.49252, 0.74669\n",
            "11: 0.50365, 0.74099\n",
            "11: 0.50501, 0.74189\n",
            "11: 0.50805, 0.73978\n",
            "11: 0.50581, 0.73377\n",
            "11: 0.49276, 0.73918\n",
            "11: 0.52166, 0.72266\n",
            "11: 0.50776, 0.74189\n",
            "12: 0.48516, 0.75060\n",
            "12: 0.50169, 0.74159\n",
            "12: 0.50224, 0.74129\n",
            "12: 0.50470, 0.74249\n",
            "12: 0.47394, 0.76052\n",
            "12: 0.50159, 0.73918\n",
            "12: 0.49537, 0.74519\n",
            "12: 0.48755, 0.75391\n",
            "12: 0.50371, 0.73377\n",
            "12: 0.48606, 0.74940\n",
            "12: 0.50812, 0.73738\n",
            "12: 0.50064, 0.74038\n",
            "12: 0.49483, 0.74219\n",
            "12: 0.52092, 0.72416\n",
            "12: 0.48299, 0.75361\n",
            "12: 0.49953, 0.74790\n",
            "12: 0.51392, 0.73678\n",
            "12: 0.50794, 0.73858\n",
            "12: 0.48905, 0.74339\n",
            "12: 0.50058, 0.73978\n",
            "12: 0.50085, 0.74970\n",
            "12: 0.50457, 0.74369\n",
            "12: 0.49499, 0.74940\n",
            "12: 0.50207, 0.74730\n",
            "12: 0.48117, 0.75691\n",
            "12: 0.48553, 0.75541\n",
            "12: 0.49548, 0.74549\n",
            "12: 0.52273, 0.72296\n",
            "12: 0.47619, 0.75481\n",
            "12: 0.49608, 0.74519\n",
            "12: 0.49637, 0.74008\n",
            "12: 0.47014, 0.76382\n",
            "12: 0.49344, 0.74880\n",
            "12: 0.50653, 0.73918\n",
            "12: 0.48450, 0.75300\n",
            "12: 0.48906, 0.74159\n",
            "12: 0.47943, 0.75451\n",
            "12: 0.50962, 0.73948\n",
            "12: 0.48475, 0.75871\n",
            "13: 0.47948, 0.75361\n",
            "13: 0.49655, 0.74940\n",
            "13: 0.48646, 0.75270\n",
            "13: 0.49806, 0.74970\n",
            "13: 0.46651, 0.75691\n",
            "13: 0.48040, 0.75901\n",
            "13: 0.47566, 0.76052\n",
            "13: 0.47998, 0.76923\n",
            "13: 0.49791, 0.74429\n",
            "13: 0.48049, 0.75541\n",
            "13: 0.48566, 0.75030\n",
            "13: 0.48802, 0.74850\n",
            "13: 0.47711, 0.75931\n",
            "13: 0.50872, 0.73708\n",
            "13: 0.47049, 0.76532\n",
            "13: 0.47721, 0.75270\n",
            "13: 0.48894, 0.74760\n",
            "13: 0.48673, 0.75090\n",
            "13: 0.47080, 0.76292\n",
            "13: 0.48742, 0.75601\n",
            "13: 0.48046, 0.75721\n",
            "13: 0.48474, 0.75391\n",
            "13: 0.48428, 0.75511\n",
            "13: 0.48501, 0.75571\n",
            "13: 0.48674, 0.75571\n",
            "13: 0.47464, 0.76953\n",
            "13: 0.47771, 0.75841\n",
            "13: 0.50002, 0.74700\n",
            "13: 0.46089, 0.76442\n",
            "13: 0.47401, 0.76082\n",
            "13: 0.47974, 0.75240\n",
            "13: 0.45760, 0.76562\n",
            "13: 0.47336, 0.76112\n",
            "13: 0.48563, 0.75451\n",
            "13: 0.47243, 0.76322\n",
            "13: 0.47886, 0.75210\n",
            "13: 0.48390, 0.74970\n",
            "13: 0.49769, 0.74820\n",
            "13: 0.49012, 0.75511\n",
            "14: 0.45496, 0.76532\n",
            "14: 0.48524, 0.74790\n",
            "14: 0.48419, 0.75541\n",
            "14: 0.47781, 0.75240\n",
            "14: 0.45107, 0.78065\n",
            "14: 0.47970, 0.75180\n",
            "14: 0.46659, 0.76502\n",
            "14: 0.46694, 0.76232\n",
            "14: 0.48814, 0.75240\n",
            "14: 0.46409, 0.75871\n",
            "14: 0.48049, 0.76052\n",
            "14: 0.46641, 0.76953\n",
            "14: 0.46730, 0.76262\n",
            "14: 0.49531, 0.75451\n",
            "14: 0.45271, 0.77434\n",
            "14: 0.48041, 0.75811\n",
            "14: 0.47389, 0.75721\n",
            "14: 0.47805, 0.75601\n",
            "14: 0.45963, 0.76923\n",
            "14: 0.47330, 0.75661\n",
            "14: 0.46928, 0.76773\n",
            "14: 0.48107, 0.76052\n",
            "14: 0.47692, 0.75931\n",
            "14: 0.50032, 0.75150\n",
            "14: 0.47460, 0.76352\n",
            "14: 0.47098, 0.76502\n",
            "14: 0.47541, 0.75511\n",
            "14: 0.48884, 0.74850\n",
            "14: 0.45465, 0.77194\n",
            "14: 0.45660, 0.77133\n",
            "14: 0.46822, 0.76593\n",
            "14: 0.44211, 0.78125\n",
            "14: 0.46541, 0.76803\n",
            "14: 0.47643, 0.76232\n",
            "14: 0.46481, 0.76803\n",
            "14: 0.46731, 0.76082\n",
            "14: 0.46461, 0.76442\n",
            "14: 0.47193, 0.76442\n",
            "14: 0.45951, 0.77404\n",
            "15: 0.44800, 0.77494\n",
            "15: 0.46912, 0.75931\n",
            "15: 0.45960, 0.76292\n",
            "15: 0.46805, 0.76172\n",
            "15: 0.43227, 0.78245\n",
            "15: 0.46458, 0.77254\n",
            "15: 0.45360, 0.77764\n",
            "15: 0.45130, 0.78335\n",
            "15: 0.46985, 0.76382\n",
            "15: 0.45187, 0.77945\n",
            "15: 0.47101, 0.76983\n",
            "15: 0.45423, 0.77374\n",
            "15: 0.45588, 0.77254\n",
            "15: 0.48013, 0.75992\n",
            "15: 0.43875, 0.78936\n",
            "15: 0.45163, 0.77644\n",
            "15: 0.46514, 0.76893\n",
            "15: 0.45559, 0.76803\n",
            "15: 0.43987, 0.77704\n",
            "15: 0.45816, 0.76803\n",
            "15: 0.45107, 0.78185\n",
            "15: 0.44803, 0.78245\n",
            "15: 0.45961, 0.77194\n",
            "15: 0.45589, 0.77554\n",
            "15: 0.45237, 0.78516\n",
            "15: 0.44699, 0.78275\n",
            "15: 0.45569, 0.77344\n",
            "15: 0.46248, 0.77434\n",
            "15: 0.43036, 0.78425\n",
            "15: 0.45036, 0.77825\n",
            "15: 0.45539, 0.76923\n",
            "15: 0.43111, 0.79207\n",
            "15: 0.45147, 0.77614\n",
            "15: 0.46178, 0.76623\n",
            "15: 0.45897, 0.76803\n",
            "15: 0.44746, 0.77855\n",
            "15: 0.44188, 0.77404\n",
            "15: 0.45902, 0.77464\n",
            "15: 0.44743, 0.77764\n",
            "16: 0.42249, 0.78696\n",
            "16: 0.45273, 0.76923\n",
            "16: 0.44304, 0.77915\n",
            "16: 0.43487, 0.78425\n",
            "16: 0.40845, 0.80889\n",
            "16: 0.45372, 0.76803\n",
            "16: 0.42090, 0.78996\n",
            "16: 0.42103, 0.79567\n",
            "16: 0.45624, 0.77554\n",
            "16: 0.42621, 0.79237\n",
            "16: 0.44688, 0.78245\n",
            "16: 0.44056, 0.78365\n",
            "16: 0.44132, 0.78125\n",
            "16: 0.45971, 0.77224\n",
            "16: 0.42063, 0.79177\n",
            "16: 0.43476, 0.78726\n",
            "16: 0.43318, 0.78516\n",
            "16: 0.45659, 0.78275\n",
            "16: 0.42781, 0.79207\n",
            "16: 0.45257, 0.76803\n",
            "16: 0.43659, 0.78816\n",
            "16: 0.42973, 0.79778\n",
            "16: 0.43258, 0.78065\n",
            "16: 0.45124, 0.78606\n",
            "16: 0.44005, 0.78966\n",
            "16: 0.42571, 0.79117\n",
            "16: 0.44436, 0.78726\n",
            "16: 0.46716, 0.76773\n",
            "16: 0.42246, 0.78936\n",
            "16: 0.41920, 0.80018\n",
            "16: 0.43124, 0.79056\n",
            "16: 0.41392, 0.79868\n",
            "16: 0.43416, 0.78065\n",
            "16: 0.44769, 0.77764\n",
            "16: 0.43294, 0.78786\n",
            "16: 0.44171, 0.77644\n",
            "16: 0.43082, 0.78365\n",
            "16: 0.44578, 0.78365\n",
            "16: 0.44155, 0.78395\n",
            "17: 0.41209, 0.79748\n",
            "17: 0.43365, 0.78726\n",
            "17: 0.42851, 0.78215\n",
            "17: 0.42493, 0.78095\n",
            "17: 0.39050, 0.81280\n",
            "17: 0.43010, 0.78516\n",
            "17: 0.41676, 0.79567\n",
            "17: 0.41360, 0.80168\n",
            "17: 0.42644, 0.79207\n",
            "17: 0.41201, 0.80078\n",
            "17: 0.43269, 0.78906\n",
            "17: 0.42109, 0.79026\n",
            "17: 0.42748, 0.78666\n",
            "17: 0.43942, 0.78516\n",
            "17: 0.40330, 0.80950\n",
            "17: 0.41161, 0.79928\n",
            "17: 0.42131, 0.78936\n",
            "17: 0.42709, 0.79627\n",
            "17: 0.40964, 0.79898\n",
            "17: 0.42625, 0.79177\n",
            "17: 0.43259, 0.79447\n",
            "17: 0.42503, 0.79778\n",
            "17: 0.42448, 0.79117\n",
            "17: 0.42716, 0.78996\n",
            "17: 0.42810, 0.79507\n",
            "17: 0.41393, 0.79657\n",
            "17: 0.42800, 0.79267\n",
            "17: 0.43811, 0.78335\n",
            "17: 0.39670, 0.80919\n",
            "17: 0.41150, 0.79748\n",
            "17: 0.41871, 0.79507\n",
            "17: 0.39631, 0.80559\n",
            "17: 0.41902, 0.79327\n",
            "17: 0.42053, 0.79627\n",
            "17: 0.41420, 0.80288\n",
            "17: 0.42236, 0.79087\n",
            "17: 0.41777, 0.78846\n",
            "17: 0.42124, 0.79177\n",
            "17: 0.40771, 0.80439\n",
            "18: 0.39903, 0.80950\n",
            "18: 0.41340, 0.79537\n",
            "18: 0.40171, 0.80198\n",
            "18: 0.41395, 0.80288\n",
            "18: 0.36817, 0.82542\n",
            "18: 0.39990, 0.81130\n",
            "18: 0.40426, 0.80198\n",
            "18: 0.40584, 0.80469\n",
            "18: 0.41669, 0.80138\n",
            "18: 0.39860, 0.80258\n",
            "18: 0.41954, 0.80168\n",
            "18: 0.39815, 0.80439\n",
            "18: 0.39418, 0.80499\n",
            "18: 0.41552, 0.80619\n",
            "18: 0.39327, 0.80919\n",
            "18: 0.39627, 0.80469\n",
            "18: 0.39476, 0.80198\n",
            "18: 0.40391, 0.80469\n",
            "18: 0.38804, 0.81340\n",
            "18: 0.41229, 0.80439\n",
            "18: 0.40704, 0.80799\n",
            "18: 0.40355, 0.80349\n",
            "18: 0.39600, 0.80649\n",
            "18: 0.40583, 0.80980\n",
            "18: 0.40901, 0.80228\n",
            "18: 0.38809, 0.80799\n",
            "18: 0.40729, 0.80829\n",
            "18: 0.41709, 0.79357\n",
            "18: 0.39642, 0.81190\n",
            "18: 0.39237, 0.80950\n",
            "18: 0.40520, 0.80108\n",
            "18: 0.37039, 0.81941\n",
            "18: 0.39117, 0.81520\n",
            "18: 0.41422, 0.79688\n",
            "18: 0.40057, 0.80980\n",
            "18: 0.39435, 0.80589\n",
            "18: 0.38845, 0.80439\n",
            "18: 0.40974, 0.80739\n",
            "18: 0.39393, 0.81190\n",
            "19: 0.38592, 0.81520\n",
            "19: 0.39897, 0.80288\n",
            "19: 0.38439, 0.81731\n",
            "19: 0.39718, 0.81040\n",
            "19: 0.35968, 0.83323\n",
            "19: 0.38384, 0.82031\n",
            "19: 0.38647, 0.81190\n",
            "19: 0.37689, 0.82181\n",
            "19: 0.38265, 0.81671\n",
            "19: 0.36911, 0.83023\n",
            "19: 0.38834, 0.80739\n",
            "19: 0.37605, 0.81791\n",
            "19: 0.38483, 0.81460\n",
            "19: 0.39537, 0.81130\n",
            "19: 0.36889, 0.82422\n",
            "19: 0.36663, 0.82512\n",
            "19: 0.38106, 0.81671\n",
            "19: 0.38039, 0.81040\n",
            "19: 0.36685, 0.82392\n",
            "19: 0.39417, 0.81430\n",
            "19: 0.38899, 0.82031\n",
            "19: 0.38391, 0.82242\n",
            "19: 0.39571, 0.80799\n",
            "19: 0.37587, 0.82031\n",
            "19: 0.37332, 0.82031\n",
            "19: 0.37010, 0.82542\n",
            "19: 0.38492, 0.81490\n",
            "19: 0.41496, 0.79597\n",
            "19: 0.37133, 0.82662\n",
            "19: 0.37594, 0.82121\n",
            "19: 0.38474, 0.81100\n",
            "19: 0.35735, 0.82662\n",
            "19: 0.36845, 0.82572\n",
            "19: 0.39016, 0.81611\n",
            "19: 0.36963, 0.82302\n",
            "19: 0.37590, 0.81851\n",
            "19: 0.37388, 0.82332\n",
            "19: 0.37455, 0.82242\n",
            "19: 0.36669, 0.83504\n",
            "20: 0.35481, 0.83444\n",
            "20: 0.37064, 0.82001\n",
            "20: 0.36708, 0.81941\n",
            "20: 0.36836, 0.82332\n",
            "20: 0.33271, 0.84345\n",
            "20: 0.36579, 0.82692\n",
            "20: 0.35643, 0.82782\n",
            "20: 0.34995, 0.83564\n",
            "20: 0.34676, 0.83444\n",
            "20: 0.35307, 0.83744\n",
            "20: 0.37224, 0.82752\n",
            "20: 0.35747, 0.82782\n",
            "20: 0.34948, 0.83323\n",
            "20: 0.37094, 0.82722\n",
            "20: 0.33625, 0.84285\n",
            "20: 0.36149, 0.83233\n",
            "20: 0.36005, 0.83053\n",
            "20: 0.36775, 0.82632\n",
            "20: 0.35307, 0.83353\n",
            "20: 0.36530, 0.82572\n",
            "20: 0.37147, 0.82873\n",
            "20: 0.34685, 0.83534\n",
            "20: 0.37343, 0.81581\n",
            "20: 0.37627, 0.82332\n",
            "20: 0.34331, 0.85096\n",
            "20: 0.35168, 0.83383\n",
            "20: 0.35771, 0.83113\n",
            "20: 0.37161, 0.82602\n",
            "20: 0.34127, 0.84165\n",
            "20: 0.33747, 0.84465\n",
            "20: 0.35416, 0.83474\n",
            "20: 0.32835, 0.84826\n",
            "20: 0.33664, 0.84615\n",
            "20: 0.36301, 0.83293\n",
            "20: 0.35622, 0.82752\n",
            "20: 0.36901, 0.82362\n",
            "20: 0.34527, 0.83924\n",
            "20: 0.35859, 0.83744\n",
            "20: 0.33343, 0.83954\n",
            "21: 0.32207, 0.84826\n",
            "21: 0.35094, 0.83474\n",
            "21: 0.34478, 0.83684\n",
            "21: 0.34664, 0.84225\n",
            "21: 0.32991, 0.84465\n",
            "21: 0.34800, 0.83474\n",
            "21: 0.32885, 0.84585\n",
            "21: 0.31471, 0.85397\n",
            "21: 0.33771, 0.84495\n",
            "21: 0.33335, 0.84345\n",
            "21: 0.34602, 0.83834\n",
            "21: 0.34123, 0.83774\n",
            "21: 0.34794, 0.84225\n",
            "21: 0.34786, 0.83804\n",
            "21: 0.34345, 0.84315\n",
            "21: 0.34838, 0.82963\n",
            "21: 0.35148, 0.84105\n",
            "21: 0.34000, 0.83954\n",
            "21: 0.34296, 0.84135\n",
            "21: 0.35678, 0.82993\n",
            "21: 0.33698, 0.84555\n",
            "21: 0.32976, 0.85337\n",
            "21: 0.34125, 0.83954\n",
            "21: 0.34866, 0.84585\n",
            "21: 0.32962, 0.84675\n",
            "21: 0.33734, 0.84916\n",
            "21: 0.33795, 0.84796\n",
            "21: 0.34964, 0.82662\n",
            "21: 0.29980, 0.86689\n",
            "21: 0.32226, 0.85337\n",
            "21: 0.33939, 0.83654\n",
            "21: 0.29449, 0.86178\n",
            "21: 0.31988, 0.84826\n",
            "21: 0.32833, 0.84495\n",
            "21: 0.32872, 0.85276\n",
            "21: 0.31643, 0.85907\n",
            "21: 0.32993, 0.85186\n",
            "21: 0.33155, 0.84315\n",
            "21: 0.31407, 0.85216\n",
            "22: 0.30227, 0.86358\n",
            "22: 0.34023, 0.84585\n",
            "22: 0.31160, 0.85186\n",
            "22: 0.33463, 0.84465\n",
            "22: 0.31488, 0.85276\n",
            "22: 0.32661, 0.85096\n",
            "22: 0.31461, 0.85487\n",
            "22: 0.31226, 0.85968\n",
            "22: 0.31927, 0.85787\n",
            "22: 0.30076, 0.86418\n",
            "22: 0.32019, 0.85006\n",
            "22: 0.29608, 0.86208\n",
            "22: 0.30809, 0.86118\n",
            "22: 0.32012, 0.85306\n",
            "22: 0.29820, 0.86298\n",
            "22: 0.29685, 0.85607\n",
            "22: 0.29979, 0.86508\n",
            "22: 0.32381, 0.85186\n",
            "22: 0.30628, 0.86508\n",
            "22: 0.31754, 0.85337\n",
            "22: 0.33126, 0.85667\n",
            "22: 0.30621, 0.85847\n",
            "22: 0.31190, 0.84946\n",
            "22: 0.31432, 0.85968\n",
            "22: 0.30049, 0.86899\n",
            "22: 0.30181, 0.85787\n",
            "22: 0.32577, 0.85517\n",
            "22: 0.32960, 0.84315\n",
            "22: 0.28266, 0.87470\n",
            "22: 0.29222, 0.86899\n",
            "22: 0.30007, 0.86719\n",
            "22: 0.28743, 0.86719\n",
            "22: 0.29457, 0.87320\n",
            "22: 0.30840, 0.85607\n",
            "22: 0.30695, 0.85968\n",
            "22: 0.29592, 0.85968\n",
            "22: 0.28000, 0.86929\n",
            "22: 0.31121, 0.85998\n",
            "22: 0.31144, 0.85817\n",
            "23: 0.29806, 0.86869\n",
            "23: 0.28242, 0.86959\n",
            "23: 0.28106, 0.87169\n",
            "23: 0.30803, 0.86478\n",
            "23: 0.27911, 0.87500\n",
            "23: 0.28960, 0.86148\n",
            "23: 0.28426, 0.87470\n",
            "23: 0.26799, 0.89153\n",
            "23: 0.30337, 0.86569\n",
            "23: 0.29308, 0.86418\n",
            "23: 0.29078, 0.86599\n",
            "23: 0.28009, 0.87290\n",
            "23: 0.26818, 0.87710\n",
            "23: 0.30193, 0.86328\n",
            "23: 0.27548, 0.87350\n",
            "23: 0.27580, 0.87169\n",
            "23: 0.28858, 0.87260\n",
            "23: 0.27995, 0.85998\n",
            "23: 0.27758, 0.88311\n",
            "23: 0.29252, 0.86178\n",
            "23: 0.29943, 0.86839\n",
            "23: 0.28425, 0.87230\n",
            "23: 0.26577, 0.87831\n",
            "23: 0.28872, 0.87290\n",
            "23: 0.28449, 0.86959\n",
            "23: 0.26595, 0.87981\n",
            "23: 0.29553, 0.86508\n",
            "23: 0.29701, 0.85397\n",
            "23: 0.27674, 0.87320\n",
            "23: 0.26959, 0.87981\n",
            "23: 0.28084, 0.87440\n",
            "23: 0.26955, 0.87500\n",
            "23: 0.26943, 0.87981\n",
            "23: 0.28424, 0.87200\n",
            "23: 0.26605, 0.88431\n",
            "23: 0.25775, 0.88191\n",
            "23: 0.26505, 0.87590\n",
            "23: 0.27707, 0.88041\n",
            "23: 0.26483, 0.87500\n",
            "24: 0.25340, 0.89303\n",
            "24: 0.26334, 0.88431\n",
            "24: 0.26146, 0.88161\n",
            "24: 0.27777, 0.87440\n",
            "24: 0.25077, 0.88552\n",
            "24: 0.26802, 0.88762\n",
            "24: 0.26484, 0.88011\n",
            "24: 0.25404, 0.88942\n",
            "24: 0.26812, 0.88732\n",
            "24: 0.25977, 0.88041\n",
            "24: 0.26484, 0.88071\n",
            "24: 0.25535, 0.88191\n",
            "24: 0.24964, 0.89333\n",
            "24: 0.28396, 0.86809\n",
            "24: 0.26563, 0.88642\n",
            "24: 0.26376, 0.87981\n",
            "24: 0.26469, 0.88191\n",
            "24: 0.26319, 0.88672\n",
            "24: 0.24132, 0.89333\n",
            "24: 0.25384, 0.88522\n",
            "24: 0.28447, 0.87350\n",
            "24: 0.26834, 0.87710\n",
            "24: 0.27800, 0.87350\n",
            "24: 0.27010, 0.87831\n",
            "24: 0.24751, 0.88732\n",
            "24: 0.25901, 0.88552\n",
            "24: 0.26501, 0.88552\n",
            "24: 0.27685, 0.87680\n",
            "24: 0.24011, 0.88972\n",
            "24: 0.25778, 0.88642\n",
            "24: 0.24372, 0.88341\n",
            "24: 0.23177, 0.89754\n",
            "24: 0.24037, 0.89483\n",
            "24: 0.25614, 0.87981\n",
            "24: 0.26554, 0.88341\n",
            "24: 0.25146, 0.88882\n",
            "24: 0.25014, 0.89123\n",
            "24: 0.26031, 0.89153\n",
            "24: 0.23132, 0.89663\n",
            "25: 0.23204, 0.89964\n",
            "25: 0.27553, 0.87710\n",
            "25: 0.23959, 0.89032\n",
            "25: 0.26593, 0.88071\n",
            "25: 0.22647, 0.90325\n",
            "25: 0.24580, 0.88792\n",
            "25: 0.22711, 0.89994\n",
            "25: 0.24556, 0.89153\n",
            "25: 0.23474, 0.89784\n",
            "25: 0.23959, 0.89663\n",
            "25: 0.25036, 0.88822\n",
            "25: 0.23356, 0.89663\n",
            "25: 0.23499, 0.89093\n",
            "25: 0.22709, 0.89694\n",
            "25: 0.22581, 0.90174\n",
            "25: 0.23840, 0.89513\n",
            "25: 0.23727, 0.89603\n",
            "25: 0.24828, 0.88942\n",
            "25: 0.23596, 0.88762\n",
            "25: 0.25954, 0.88852\n",
            "25: 0.25954, 0.88522\n",
            "25: 0.25025, 0.89213\n",
            "25: 0.25635, 0.88582\n",
            "25: 0.25321, 0.88642\n",
            "25: 0.23546, 0.89063\n",
            "25: 0.24406, 0.89183\n",
            "25: 0.22220, 0.90385\n",
            "25: 0.27384, 0.87680\n",
            "25: 0.25868, 0.88492\n",
            "25: 0.22320, 0.90054\n",
            "25: 0.23892, 0.89303\n",
            "25: 0.21050, 0.91136\n",
            "25: 0.23195, 0.90084\n",
            "25: 0.22024, 0.90445\n",
            "25: 0.24714, 0.89543\n",
            "25: 0.24251, 0.89393\n",
            "25: 0.23963, 0.89333\n",
            "25: 0.22584, 0.90174\n",
            "25: 0.21866, 0.89844\n",
            "26: 0.20727, 0.91226\n",
            "26: 0.23334, 0.90024\n",
            "26: 0.22139, 0.90415\n",
            "26: 0.25400, 0.88792\n",
            "26: 0.20817, 0.91076\n",
            "26: 0.23436, 0.89333\n",
            "26: 0.22694, 0.90415\n",
            "26: 0.22613, 0.89934\n",
            "26: 0.22670, 0.89844\n",
            "26: 0.20752, 0.91256\n",
            "26: 0.21862, 0.89844\n",
            "26: 0.22155, 0.90505\n",
            "26: 0.22036, 0.90595\n",
            "26: 0.22053, 0.90505\n",
            "26: 0.21000, 0.90294\n",
            "26: 0.22353, 0.90385\n",
            "26: 0.22558, 0.90114\n",
            "26: 0.24198, 0.89573\n",
            "26: 0.21861, 0.90565\n",
            "26: 0.23678, 0.89363\n",
            "26: 0.23024, 0.89543\n",
            "26: 0.24280, 0.89483\n",
            "26: 0.22734, 0.89724\n",
            "26: 0.23365, 0.90445\n",
            "26: 0.22734, 0.90505\n",
            "26: 0.20820, 0.91106\n",
            "26: 0.22006, 0.90655\n",
            "26: 0.22307, 0.89904\n",
            "26: 0.21253, 0.90625\n",
            "26: 0.20449, 0.91346\n",
            "26: 0.20856, 0.90685\n",
            "26: 0.20090, 0.91466\n",
            "26: 0.20584, 0.90805\n",
            "26: 0.21405, 0.91286\n",
            "26: 0.21941, 0.90625\n",
            "26: 0.22593, 0.90475\n",
            "26: 0.21554, 0.90204\n",
            "26: 0.23151, 0.90234\n",
            "26: 0.19447, 0.91737\n",
            "27: 0.18664, 0.91677\n",
            "27: 0.20261, 0.91196\n",
            "27: 0.20736, 0.90805\n",
            "27: 0.21260, 0.91166\n",
            "27: 0.20587, 0.90745\n",
            "27: 0.21357, 0.91226\n",
            "27: 0.20396, 0.91286\n",
            "27: 0.21124, 0.90805\n",
            "27: 0.20046, 0.90895\n",
            "27: 0.18364, 0.91977\n",
            "27: 0.21144, 0.90475\n",
            "27: 0.18944, 0.92188\n",
            "27: 0.18852, 0.92037\n",
            "27: 0.19056, 0.91917\n",
            "27: 0.18692, 0.91827\n",
            "27: 0.19456, 0.91587\n",
            "27: 0.21062, 0.91196\n",
            "27: 0.20619, 0.90895\n",
            "27: 0.19882, 0.91166\n",
            "27: 0.20608, 0.91136\n",
            "27: 0.21720, 0.90204\n",
            "27: 0.19466, 0.91346\n",
            "27: 0.20436, 0.90595\n",
            "27: 0.19452, 0.91406\n",
            "27: 0.20825, 0.91346\n",
            "27: 0.19657, 0.91016\n",
            "27: 0.21085, 0.91286\n",
            "27: 0.21928, 0.90475\n",
            "27: 0.20213, 0.91046\n",
            "27: 0.19979, 0.91316\n",
            "27: 0.19253, 0.91767\n",
            "27: 0.17537, 0.92548\n",
            "27: 0.19012, 0.91737\n",
            "27: 0.19630, 0.91647\n",
            "27: 0.19933, 0.91496\n",
            "27: 0.20661, 0.90775\n",
            "27: 0.21262, 0.90595\n",
            "27: 0.19412, 0.91947\n",
            "27: 0.18955, 0.91917\n",
            "28: 0.19673, 0.91707\n",
            "28: 0.19854, 0.91166\n",
            "28: 0.19520, 0.91857\n",
            "28: 0.21451, 0.90625\n",
            "28: 0.18287, 0.92097\n",
            "28: 0.21159, 0.90595\n",
            "28: 0.18047, 0.92608\n",
            "28: 0.17161, 0.93389\n",
            "28: 0.19989, 0.91526\n",
            "28: 0.19304, 0.91136\n",
            "28: 0.18425, 0.91857\n",
            "28: 0.17772, 0.92819\n",
            "28: 0.19268, 0.91376\n",
            "28: 0.18083, 0.92548\n",
            "28: 0.17945, 0.92758\n",
            "28: 0.18018, 0.92188\n",
            "28: 0.17462, 0.91526\n",
            "28: 0.20229, 0.91887\n",
            "28: 0.19432, 0.91106\n",
            "28: 0.18821, 0.91767\n",
            "28: 0.17888, 0.92458\n",
            "28: 0.19001, 0.91436\n",
            "28: 0.18623, 0.92368\n",
            "28: 0.18760, 0.91917\n",
            "28: 0.18640, 0.92188\n",
            "28: 0.16974, 0.92728\n",
            "28: 0.18696, 0.92007\n",
            "28: 0.21656, 0.90475\n",
            "28: 0.16481, 0.93239\n",
            "28: 0.16898, 0.93119\n",
            "28: 0.17569, 0.92518\n",
            "28: 0.17133, 0.92458\n",
            "28: 0.17239, 0.92458\n",
            "28: 0.18507, 0.92127\n",
            "28: 0.18619, 0.92608\n",
            "28: 0.18612, 0.91647\n",
            "28: 0.18128, 0.92007\n",
            "28: 0.19648, 0.91617\n",
            "28: 0.17903, 0.92668\n",
            "29: 0.30859, 0.87260\n",
            "29: 0.27020, 0.87951\n",
            "29: 0.21842, 0.89904\n",
            "29: 0.21680, 0.90294\n",
            "29: 0.19817, 0.91436\n",
            "29: 0.20937, 0.91016\n",
            "29: 0.20048, 0.91436\n",
            "29: 0.19839, 0.91406\n",
            "29: 0.20109, 0.91406\n",
            "29: 0.18897, 0.91977\n",
            "29: 0.18980, 0.91797\n",
            "29: 0.17810, 0.91947\n",
            "29: 0.18340, 0.92548\n",
            "29: 0.17776, 0.92067\n",
            "29: 0.18592, 0.91406\n",
            "29: 0.17250, 0.92608\n",
            "29: 0.20092, 0.91647\n",
            "29: 0.17791, 0.92608\n",
            "29: 0.17050, 0.92548\n",
            "29: 0.19871, 0.91556\n",
            "29: 0.16515, 0.92758\n",
            "29: 0.17062, 0.92578\n",
            "29: 0.19153, 0.92308\n",
            "29: 0.19197, 0.91887\n",
            "29: 0.17905, 0.92338\n",
            "29: 0.16536, 0.93149\n",
            "29: 0.15569, 0.93810\n",
            "29: 0.19875, 0.91256\n",
            "29: 0.17364, 0.92879\n",
            "29: 0.17260, 0.92999\n",
            "29: 0.18066, 0.92157\n",
            "29: 0.15706, 0.93329\n",
            "29: 0.16482, 0.92939\n",
            "29: 0.15321, 0.93570\n",
            "29: 0.17263, 0.92758\n",
            "29: 0.15343, 0.93630\n",
            "29: 0.15955, 0.93900\n",
            "29: 0.16395, 0.92728\n",
            "29: 0.17553, 0.92849\n",
            "30: 0.17699, 0.92188\n",
            "30: 0.18133, 0.92248\n",
            "30: 0.14684, 0.93720\n",
            "30: 0.16886, 0.92879\n",
            "30: 0.14913, 0.93930\n",
            "30: 0.15509, 0.93600\n",
            "30: 0.17620, 0.92067\n",
            "30: 0.16884, 0.92819\n",
            "30: 0.16873, 0.92638\n",
            "30: 0.15710, 0.93510\n",
            "30: 0.15478, 0.93419\n",
            "30: 0.14873, 0.93570\n",
            "30: 0.15077, 0.93600\n",
            "30: 0.15059, 0.93780\n",
            "30: 0.14420, 0.93900\n",
            "30: 0.15148, 0.94020\n",
            "30: 0.15554, 0.92969\n",
            "30: 0.17194, 0.92428\n",
            "30: 0.17294, 0.92969\n",
            "30: 0.16597, 0.92698\n",
            "30: 0.15868, 0.93840\n",
            "30: 0.15807, 0.93630\n",
            "30: 0.14604, 0.93720\n",
            "30: 0.15571, 0.93810\n",
            "30: 0.14545, 0.93630\n",
            "30: 0.14663, 0.93930\n",
            "30: 0.13917, 0.93960\n",
            "30: 0.15779, 0.93630\n",
            "30: 0.13609, 0.93810\n",
            "30: 0.16985, 0.92638\n",
            "30: 0.14308, 0.94441\n",
            "30: 0.14526, 0.93570\n",
            "30: 0.15154, 0.94321\n",
            "30: 0.14577, 0.93960\n",
            "30: 0.15174, 0.93750\n",
            "30: 0.13964, 0.94231\n",
            "30: 0.16320, 0.93570\n",
            "30: 0.14390, 0.94291\n",
            "30: 0.14107, 0.94231\n",
            "31: 0.14912, 0.93600\n",
            "31: 0.14537, 0.93810\n",
            "31: 0.13853, 0.93930\n",
            "31: 0.14662, 0.93840\n",
            "31: 0.14319, 0.93750\n",
            "31: 0.16057, 0.93269\n",
            "31: 0.15362, 0.93389\n",
            "31: 0.15181, 0.93960\n",
            "31: 0.14840, 0.93419\n",
            "31: 0.13622, 0.94351\n",
            "31: 0.14903, 0.94050\n",
            "31: 0.14049, 0.93870\n",
            "31: 0.13983, 0.93960\n",
            "31: 0.13901, 0.93900\n",
            "31: 0.12278, 0.94712\n",
            "31: 0.12750, 0.94591\n",
            "31: 0.14221, 0.94231\n",
            "31: 0.15075, 0.93419\n",
            "31: 0.13033, 0.94561\n",
            "31: 0.15294, 0.93660\n",
            "31: 0.14986, 0.94020\n",
            "31: 0.15038, 0.93960\n",
            "31: 0.15163, 0.93720\n",
            "31: 0.15170, 0.93990\n",
            "31: 0.13704, 0.94531\n",
            "31: 0.13822, 0.94862\n",
            "31: 0.14417, 0.93960\n",
            "31: 0.15406, 0.92668\n",
            "31: 0.13244, 0.94201\n",
            "31: 0.13243, 0.94141\n",
            "31: 0.13186, 0.94772\n",
            "31: 0.13405, 0.94291\n",
            "31: 0.13763, 0.94321\n",
            "31: 0.15151, 0.93720\n",
            "31: 0.13406, 0.94201\n",
            "31: 0.13508, 0.94411\n",
            "31: 0.13767, 0.94201\n",
            "31: 0.14617, 0.93840\n",
            "31: 0.12672, 0.94922\n",
            "32: 0.14068, 0.94171\n",
            "32: 0.14251, 0.94171\n",
            "32: 0.13451, 0.94291\n",
            "32: 0.12373, 0.95162\n",
            "32: 0.12808, 0.94802\n",
            "32: 0.14816, 0.93960\n",
            "32: 0.14482, 0.93480\n",
            "32: 0.14489, 0.94441\n",
            "32: 0.14830, 0.93480\n",
            "32: 0.12962, 0.94561\n",
            "32: 0.13734, 0.94381\n",
            "32: 0.14699, 0.93780\n",
            "32: 0.13644, 0.94081\n",
            "32: 0.12985, 0.95042\n",
            "32: 0.12443, 0.94531\n",
            "32: 0.11409, 0.95733\n",
            "32: 0.12929, 0.94261\n",
            "32: 0.13206, 0.94621\n",
            "32: 0.13372, 0.94591\n",
            "32: 0.14286, 0.93870\n",
            "32: 0.14184, 0.94171\n",
            "32: 0.14314, 0.93900\n",
            "32: 0.13285, 0.94261\n",
            "32: 0.13708, 0.94381\n",
            "32: 0.13430, 0.94591\n",
            "32: 0.14671, 0.93840\n",
            "32: 0.11945, 0.95012\n",
            "32: 0.14462, 0.94050\n",
            "32: 0.13775, 0.94231\n",
            "32: 0.13594, 0.94201\n",
            "32: 0.12508, 0.94892\n",
            "32: 0.11644, 0.95222\n",
            "32: 0.13663, 0.94561\n",
            "32: 0.14493, 0.93870\n",
            "32: 0.13041, 0.94531\n",
            "32: 0.11420, 0.95493\n",
            "32: 0.13115, 0.94591\n",
            "32: 0.14312, 0.94381\n",
            "32: 0.13399, 0.94591\n",
            "33: 0.12743, 0.94381\n",
            "33: 0.12383, 0.95162\n",
            "33: 0.13137, 0.94321\n",
            "33: 0.13351, 0.94471\n",
            "33: 0.11186, 0.95132\n",
            "33: 0.11902, 0.94712\n",
            "33: 0.12590, 0.94501\n",
            "33: 0.13200, 0.94802\n",
            "33: 0.15370, 0.94081\n",
            "33: 0.13870, 0.93840\n",
            "33: 0.13128, 0.94381\n",
            "33: 0.11960, 0.94952\n",
            "33: 0.11323, 0.95763\n",
            "33: 0.12824, 0.94471\n",
            "33: 0.11576, 0.94591\n",
            "33: 0.11322, 0.95433\n",
            "33: 0.11416, 0.95192\n",
            "33: 0.11057, 0.95673\n",
            "33: 0.12528, 0.95072\n",
            "33: 0.14063, 0.94261\n",
            "33: 0.14196, 0.94020\n",
            "33: 0.15286, 0.94050\n",
            "33: 0.14238, 0.93810\n",
            "33: 0.13083, 0.94321\n",
            "33: 0.11744, 0.95102\n",
            "33: 0.12057, 0.95313\n",
            "33: 0.12382, 0.94952\n",
            "33: 0.14323, 0.94201\n",
            "33: 0.11839, 0.95132\n",
            "33: 0.13387, 0.94351\n",
            "33: 0.11437, 0.95523\n",
            "33: 0.11175, 0.95042\n",
            "33: 0.11935, 0.95042\n",
            "33: 0.10983, 0.95733\n",
            "33: 0.12250, 0.94832\n",
            "33: 0.11734, 0.95553\n",
            "33: 0.13186, 0.94862\n",
            "33: 0.12487, 0.94832\n",
            "33: 0.11508, 0.95673\n",
            "34: 0.10684, 0.95373\n",
            "34: 0.10696, 0.95553\n",
            "34: 0.11113, 0.95192\n",
            "34: 0.12662, 0.94712\n",
            "34: 0.11970, 0.95192\n",
            "34: 0.12777, 0.94982\n",
            "34: 0.14410, 0.94471\n",
            "34: 0.12860, 0.94141\n",
            "34: 0.13539, 0.94651\n",
            "34: 0.13513, 0.93960\n",
            "34: 0.11398, 0.95282\n",
            "34: 0.13068, 0.94651\n",
            "34: 0.11742, 0.95493\n",
            "34: 0.12253, 0.95613\n",
            "34: 0.09538, 0.95853\n",
            "34: 0.11610, 0.95493\n",
            "34: 0.11769, 0.95343\n",
            "34: 0.11371, 0.95222\n",
            "34: 0.12528, 0.94561\n",
            "34: 0.11664, 0.95102\n",
            "34: 0.13258, 0.94321\n",
            "34: 0.12735, 0.94892\n",
            "34: 0.11663, 0.95192\n",
            "34: 0.12124, 0.95162\n",
            "34: 0.10874, 0.96094\n",
            "34: 0.12063, 0.95162\n",
            "34: 0.10581, 0.95913\n",
            "34: 0.12251, 0.94802\n",
            "34: 0.10504, 0.95823\n",
            "34: 0.12623, 0.94441\n",
            "34: 0.12118, 0.95042\n",
            "34: 0.11209, 0.95523\n",
            "34: 0.12483, 0.95192\n",
            "34: 0.10418, 0.95523\n",
            "34: 0.11615, 0.94802\n",
            "34: 0.11908, 0.95072\n",
            "34: 0.09972, 0.96364\n",
            "34: 0.12175, 0.94922\n",
            "34: 0.10482, 0.95883\n",
            "35: 0.09270, 0.96244\n",
            "35: 0.09553, 0.96004\n",
            "35: 0.10499, 0.95313\n",
            "35: 0.11620, 0.95282\n",
            "35: 0.47030, 0.84345\n",
            "35: 0.39593, 0.84675\n",
            "35: 0.31199, 0.86929\n",
            "35: 0.26253, 0.89032\n",
            "35: 0.24407, 0.90114\n",
            "35: 0.21405, 0.90595\n",
            "35: 0.19440, 0.91647\n",
            "35: 0.17506, 0.92638\n",
            "35: 0.16814, 0.92698\n",
            "35: 0.17309, 0.92398\n",
            "35: 0.15904, 0.93450\n",
            "35: 0.14735, 0.93990\n",
            "35: 0.14188, 0.93720\n",
            "35: 0.14858, 0.93419\n",
            "35: 0.14429, 0.93900\n",
            "35: 0.12795, 0.94862\n",
            "35: 0.14735, 0.94411\n",
            "35: 0.14647, 0.93870\n",
            "35: 0.13903, 0.93720\n",
            "35: 0.13377, 0.94561\n",
            "35: 0.13024, 0.93990\n",
            "35: 0.12931, 0.94651\n",
            "35: 0.12702, 0.94772\n",
            "35: 0.14585, 0.93960\n",
            "35: 0.13185, 0.94621\n",
            "35: 0.13663, 0.93900\n",
            "35: 0.13483, 0.94381\n",
            "35: 0.11295, 0.95373\n",
            "35: 0.10740, 0.95673\n",
            "35: 0.12005, 0.95132\n",
            "35: 0.13540, 0.94591\n",
            "35: 0.11453, 0.95162\n",
            "35: 0.11590, 0.95433\n",
            "35: 0.10574, 0.95553\n",
            "35: 0.11771, 0.95313\n",
            "36: 0.11514, 0.95192\n",
            "36: 0.11009, 0.95282\n",
            "36: 0.11284, 0.95643\n",
            "36: 0.11962, 0.94892\n",
            "36: 0.11024, 0.95343\n",
            "36: 0.10919, 0.95643\n",
            "36: 0.10618, 0.95853\n",
            "36: 0.10706, 0.95793\n",
            "36: 0.11249, 0.95343\n",
            "36: 0.12544, 0.94561\n",
            "36: 0.11026, 0.95102\n",
            "36: 0.10754, 0.96244\n",
            "36: 0.11066, 0.95553\n",
            "36: 0.10255, 0.95703\n",
            "36: 0.10546, 0.95853\n",
            "36: 0.10947, 0.95643\n",
            "36: 0.10247, 0.96094\n",
            "36: 0.10827, 0.95583\n",
            "36: 0.09699, 0.96124\n",
            "36: 0.11819, 0.94712\n",
            "36: 0.11383, 0.95613\n",
            "36: 0.09321, 0.96364\n",
            "36: 0.11254, 0.95433\n",
            "36: 0.10948, 0.95733\n",
            "36: 0.10969, 0.95793\n",
            "36: 0.09748, 0.96154\n",
            "36: 0.10441, 0.96004\n",
            "36: 0.10624, 0.95403\n",
            "36: 0.09506, 0.96184\n",
            "36: 0.09266, 0.96214\n",
            "36: 0.10014, 0.95313\n",
            "36: 0.09864, 0.95974\n",
            "36: 0.10124, 0.95673\n",
            "36: 0.10441, 0.96034\n",
            "36: 0.09678, 0.95913\n",
            "36: 0.10720, 0.95823\n",
            "36: 0.08892, 0.96484\n",
            "36: 0.09503, 0.96244\n",
            "36: 0.11208, 0.95763\n",
            "37: 0.08560, 0.96815\n",
            "37: 0.09116, 0.96274\n",
            "37: 0.09404, 0.96184\n",
            "37: 0.10872, 0.95313\n",
            "37: 0.09817, 0.96064\n",
            "37: 0.09657, 0.95974\n",
            "37: 0.09973, 0.96064\n",
            "37: 0.09010, 0.96094\n",
            "37: 0.10353, 0.95583\n",
            "37: 0.09703, 0.95823\n",
            "37: 0.09010, 0.96214\n",
            "37: 0.08222, 0.96725\n",
            "37: 0.08836, 0.96304\n",
            "37: 0.08548, 0.96424\n",
            "37: 0.08509, 0.96454\n",
            "37: 0.08874, 0.96244\n",
            "37: 0.09025, 0.96214\n",
            "37: 0.10937, 0.95913\n",
            "37: 0.10354, 0.95703\n",
            "37: 0.10818, 0.95192\n",
            "37: 0.09718, 0.96034\n",
            "37: 0.10513, 0.95643\n",
            "37: 0.10528, 0.95373\n",
            "37: 0.09834, 0.95673\n",
            "37: 0.10867, 0.95974\n",
            "37: 0.08277, 0.96665\n",
            "37: 0.09783, 0.96244\n",
            "37: 0.09533, 0.95974\n",
            "37: 0.09346, 0.96184\n",
            "37: 0.07216, 0.97266\n",
            "37: 0.09375, 0.95974\n",
            "37: 0.07851, 0.96845\n",
            "37: 0.08712, 0.96394\n",
            "37: 0.08199, 0.96484\n",
            "37: 0.09759, 0.96154\n",
            "37: 0.09156, 0.96274\n",
            "37: 0.09779, 0.96364\n",
            "37: 0.09709, 0.95913\n",
            "37: 0.08343, 0.96905\n",
            "38: 0.08519, 0.96034\n",
            "38: 0.09730, 0.96064\n",
            "38: 0.07089, 0.97296\n",
            "38: 0.08265, 0.96544\n",
            "38: 0.07917, 0.97025\n",
            "38: 0.10118, 0.95974\n",
            "38: 0.09327, 0.96274\n",
            "38: 0.08813, 0.96635\n",
            "38: 0.09352, 0.96124\n",
            "38: 0.10332, 0.95823\n",
            "38: 0.09183, 0.96274\n",
            "38: 0.09589, 0.96364\n",
            "38: 0.09785, 0.96124\n",
            "38: 0.08871, 0.96514\n",
            "38: 0.08790, 0.96605\n",
            "38: 0.08890, 0.96004\n",
            "38: 0.08068, 0.96334\n",
            "38: 0.08771, 0.96244\n",
            "38: 0.09908, 0.96304\n",
            "38: 0.08859, 0.96484\n",
            "38: 0.09393, 0.95913\n",
            "38: 0.09653, 0.96424\n",
            "38: 0.09649, 0.96184\n",
            "38: 0.09856, 0.96124\n",
            "38: 0.09590, 0.96364\n",
            "38: 0.09846, 0.95944\n",
            "38: 0.09465, 0.95974\n",
            "38: 0.09091, 0.96274\n",
            "38: 0.08271, 0.96484\n",
            "38: 0.08690, 0.96635\n",
            "38: 0.07656, 0.96665\n",
            "38: 0.07619, 0.97236\n",
            "38: 0.08069, 0.96544\n",
            "38: 0.08745, 0.96635\n",
            "38: 0.08498, 0.96364\n",
            "38: 0.07854, 0.96965\n",
            "38: 0.08420, 0.96785\n",
            "38: 0.09335, 0.96575\n",
            "38: 0.07040, 0.97115\n",
            "39: 0.06084, 0.97686\n",
            "39: 0.07952, 0.96935\n",
            "39: 0.09235, 0.96394\n",
            "39: 0.08082, 0.96635\n",
            "39: 0.07986, 0.96875\n",
            "39: 0.09136, 0.96304\n",
            "39: 0.07780, 0.96665\n",
            "39: 0.08790, 0.96695\n",
            "39: 0.09057, 0.96785\n",
            "39: 0.08766, 0.96424\n",
            "39: 0.08330, 0.96514\n",
            "39: 0.08274, 0.96845\n",
            "39: 0.08933, 0.96364\n",
            "39: 0.07872, 0.96905\n",
            "39: 0.09160, 0.96154\n",
            "39: 0.09983, 0.95793\n",
            "39: 0.09734, 0.96214\n",
            "39: 0.09277, 0.96514\n",
            "39: 0.09457, 0.95913\n",
            "39: 0.08655, 0.96665\n",
            "39: 0.08400, 0.96454\n",
            "39: 0.08909, 0.96484\n",
            "39: 0.08105, 0.96514\n",
            "39: 0.09130, 0.96124\n",
            "39: 0.08528, 0.96605\n",
            "39: 0.08689, 0.96454\n",
            "39: 0.07507, 0.96815\n",
            "39: 0.08677, 0.96424\n",
            "39: 0.08591, 0.96484\n",
            "39: 0.08903, 0.96575\n",
            "39: 0.08115, 0.96484\n",
            "39: 0.08726, 0.96304\n",
            "39: 0.07339, 0.96965\n",
            "39: 0.08792, 0.96484\n",
            "39: 0.08816, 0.96875\n",
            "39: 0.08515, 0.96665\n",
            "39: 0.07876, 0.97476\n",
            "39: 0.08998, 0.96605\n",
            "39: 0.06965, 0.97296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "-YuJnZ8uZjed",
        "outputId": "1a21d8d8-18ff-45c5-be63-d6b5353606e9"
      },
      "source": [
        "import os\r\n",
        "from typing import Tuple, Sequence, Callable\r\n",
        "import csv\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from PIL import Image\r\n",
        "import torch\r\n",
        "import torch.optim as optim\r\n",
        "from torch import nn, Tensor\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "# from torchinfo import summary\r\n",
        "\r\n",
        "from torchvision import transforms, utils\r\n",
        "from torchvision.models import resnet50\r\n",
        "from skimage import io, transform\r\n",
        "\r\n",
        "class MnistDataset(Dataset):\r\n",
        "    def __init__(self,dir: os.PathLike,image_ids: os.PathLike,transforms: Sequence[Callable]) -> None:\r\n",
        "        self.dir = dir\r\n",
        "        self.transforms = transforms\r\n",
        "\r\n",
        "        self.labels = {}\r\n",
        "        with open(image_ids, 'r') as f:\r\n",
        "            reader = csv.reader(f)\r\n",
        "            next(reader)\r\n",
        "            for row in reader:\r\n",
        "                self.labels[int(row[0])] = list(map(int, row[1:]))\r\n",
        "\r\n",
        "        self.image_ids = list(self.labels.keys())\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self.image_ids)\r\n",
        "\r\n",
        "    def __getitem__(self, index: int) -> Tuple[Tensor]:\r\n",
        "        image_id = self.image_ids[index]\r\n",
        "        image = Image.open(\r\n",
        "            os.path.join(\r\n",
        "                self.dir, f'{str(image_id).zfill(5)}.png')).resize((128,128)).convert('RGB')\r\n",
        "        target = np.array(self.labels.get(image_id)).astype(np.float32)\r\n",
        "\r\n",
        "        if self.transforms is not None:\r\n",
        "            image = self.transforms(image)\r\n",
        "\r\n",
        "        return image, target\r\n",
        "\r\n",
        "transforms_train = transforms.Compose([\r\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\r\n",
        "    transforms.RandomVerticalFlip(p=0.5),\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize(\r\n",
        "        [0.485, 0.456, 0.406],\r\n",
        "        [0.229, 0.224, 0.225]\r\n",
        "    )\r\n",
        "])\r\n",
        "\r\n",
        "transforms_test = transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize(\r\n",
        "        [0.485, 0.456, 0.406],\r\n",
        "        [0.229, 0.224, 0.225]\r\n",
        "    )\r\n",
        "])\r\n",
        "\r\n",
        "trainset = MnistDataset('/content/drive/My Drive/dirty_mnist_/dirty_mnist_2nd/', '/content/drive/My Drive/dirty_mnist_/dirty_mnist_2nd_answer.csv', transforms_train)\r\n",
        "testset = MnistDataset('/content/drive/My Drive/dirty_mnist_/test_dirty_mnist_2nd/', '/content/drive/My Drive/dirty_mnist_/sample_submission.csv', transforms_test)\r\n",
        "\r\n",
        "train_loader = DataLoader(trainset, batch_size=128, num_workers=8)\r\n",
        "test_loader = DataLoader(testset, batch_size=32, num_workers=4)\r\n",
        "\r\n",
        "class MnistModel(nn.Module):\r\n",
        "    def __init__(self) -> None:\r\n",
        "        super().__init__()\r\n",
        "        self.resnet = resnet50(pretrained=True)\r\n",
        "        self.classifier = nn.Linear(1000, 26)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.resnet(x)\r\n",
        "        x = self.classifier(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "model = MnistModel().to(device)\r\n",
        "# print(summary(model, input_size=(1, 3, 128, 128), verbose=1))\r\n",
        "\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\r\n",
        "criterion = nn.MultiLabelSoftMarginLoss()\r\n",
        "\r\n",
        "num_epochs = 30\r\n",
        "model.train()\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    for i, (images, targets) in enumerate(train_loader):\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        images = images.to(device)\r\n",
        "        targets = targets.to(device)\r\n",
        "\r\n",
        "        outputs = model(images)\r\n",
        "        loss = criterion(outputs, targets)\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        if (i+1) % 10 == 0:\r\n",
        "            outputs = outputs > 0.5\r\n",
        "            acc = (outputs == targets).float().mean()\r\n",
        "            torch.save(model.state_dict(), os.path.join('/content/drive/My Drive/dirty_mnist_/checkpoint/', f'pretrained_model2_{epoch}.pth'))\r\n",
        "            print(f'{epoch}: {loss.item():.5f}, {acc.item():.5f}')\r\n",
        "\r\n",
        "submit = pd.read_csv('/content/drive/My Drive/dirty_mnist_/sample_submission.csv')\r\n",
        "# model2 = MnistModel().to(device)\r\n",
        "# model2.load_state_dict(torch.load('/content/drive/My Drive/dirty_mnist_/checkpoint/pretrained_model19.pth'))\r\n",
        "model.eval()\r\n",
        "batch_size = test_loader.batch_size\r\n",
        "batch_index = 0\r\n",
        "for i, (images, targets) in enumerate(test_loader):\r\n",
        "    images = images.to(device)\r\n",
        "    targets = targets.to(device)\r\n",
        "    outputs = model(images)\r\n",
        "    outputs = outputs > 0.5\r\n",
        "    batch_index = i * batch_size\r\n",
        "    submit.iloc[batch_index:batch_index+batch_size, 1:] = \\\r\n",
        "        outputs.long().squeeze(0).detach().cpu().numpy()\r\n",
        "    \r\n",
        "submit.to_csv('/content/drive/My Drive/dirty_mnist_/submission_torch7.csv', index=False)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-a75829f86445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMnistModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;31m# print(summary(model, input_size=(1, 3, 128, 128), verbose=1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-a75829f86445>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mresnet50\u001b[0;34m(pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \"\"\"\n\u001b[1;32m    264\u001b[0m     return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n\u001b[0;32m--> 265\u001b[0;31m                    **kwargs)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_resnet\u001b[0;34m(arch, block, layers, pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         state_dict = load_state_dict_from_url(model_urls[arch],\n\u001b[0;32m--> 227\u001b[0;31m                                               progress=progress)\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_legacy_zip_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_zip_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;31m# only if offset is zero we can attempt the legacy tar file loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlegacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTarError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mlegacy_load\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m             \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'storages'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'storages'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 \u001b[0mnum_storages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2052\u001b[0m             self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n\u001b[1;32m   2053\u001b[0m                                  \u001b[0mset_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2054\u001b[0;31m                                  numeric_owner=numeric_owner)\n\u001b[0m\u001b[1;32m   2055\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2056\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrorlevel\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2124\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2125\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mmakefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2171\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2173\u001b[0;31m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmakeunknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremainder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW2NPDbJ8QAz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23TOba33L4qf",
        "outputId": "d774e593-7178-4252-fc7a-e300a7a129b3"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Feb 12 02:37:20 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "메모장에서 GPU를 사용하려면 런타임 &#62; 런타임 유형 변경 메뉴를 선택한 다음 하드웨어 가속기 드롭다운을 GPU로 설정하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## 추가 메모리\n",
        "\n",
        "<p>Colab Pro를 구독하면 사용 가능한 경우 고용량 메모리 VM에 액세스할 수 있습니다. 고용량 메모리 런타임을 사용하도록 메모장 환경설정을 지정하려면 런타임 &#62; '런타임 유형 변경' 메뉴를 선택한 다음 런타임 구성 드롭다운에서 고용량 RAM을 선택하세요.</p>\n",
        "<p>다음 코드를 실행하여 언제든지 사용 가능한 메모리 용량을 확인할 수 있습니다.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}